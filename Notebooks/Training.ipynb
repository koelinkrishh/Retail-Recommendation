{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1caeb3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ed2bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Library to import code from other notebook\n",
    "# !pip install nbimporter\n",
    "\n",
    "## Importing all function from model\n",
    "from Model import Retail_Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7bd402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading datasets\n",
    "df = pd.read_parquet('../Data/data_with_features.parquet')\n",
    "customer = pd.read_pickle('../Data/customer_history.pkl')\n",
    "items = pd.read_pickle('../Data/item_summary.pkl')\n",
    "baskets = pd.read_pickle('../Data/baskets.pkl')\n",
    "itemsets = joblib.load('../Models/itemsets.joblib')\n",
    "rules = pd.read_pickle('../Data/rules.pkl')\n",
    "vectorizer = joblib.load('../Models/vectorizer.joblib')\n",
    "\n",
    "# coefficients to train\n",
    "params = {'alpha': 1.0, 'beta': 1.0, 'delta': 1.0, 'eta': 1.0, 'gamma': 1.0, 'epsilon': 1.0, 'd_effect': 0.5}\n",
    "\n",
    "# Calculating parameters\n",
    "Today = pd.Timestamp(df['Purchase Date'].max()) + pd.DateOffset(days=1)\n",
    "Time_period = (df['Purchase Date'].max() - df['Purchase Date'].min()).days\n",
    "d_effect = 0.5 # decay factor\n",
    "EPS = 1e-9\n",
    "DEFAULT_HALF_LIFE_DAYS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32d637ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = Retail_Recommendation(\n",
    "    items_data = items,\n",
    "    customer_data = customer,\n",
    "    rules = rules,\n",
    "    initial_weights = None,\n",
    "    d_effect = 1,\n",
    "    current_date = Today,\n",
    "    Time_period = 150,\n",
    "    filter_items = False,\n",
    "    include_description = True,\n",
    "    iteration_bar = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b63df",
   "metadata": {},
   "source": [
    "### Part-5: Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb2564c",
   "metadata": {},
   "source": [
    "We have our perfect recommendation function. <br>\n",
    "Now, we will Fine-tune various constants and coefficients involved in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3bbb29",
   "metadata": {},
   "source": [
    "But for Training this model, Firstly we need some form of data with which we can evaluate our model by categorizing loss function.\n",
    "\n",
    "**Solution**: <br>\n",
    "We can take each baskets and try to predict its last items. This convert it into a supervised classification problem.\n",
    "If prediction is correct, then we can say that our model is performing well.\n",
    "\n",
    "** Loss Function: ** Categorical cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6204bc32",
   "metadata": {},
   "source": [
    "#### 1) Baskets input and last item for recommendation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53caf736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carts shape:  (19857, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Num products</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24230</th>\n",
       "      <td>559989</td>\n",
       "      <td>13319</td>\n",
       "      <td>[23209, 23245, 23298, 47566, 22993, 22328, 223...</td>\n",
       "      <td>[lunch|bag|doiley|pattern, set|of|3|regency|ca...</td>\n",
       "      <td>[1.65, 4.95, 4.95, 4.95, 1.25, 2.95, 2.95, 1.2...</td>\n",
       "      <td>10</td>\n",
       "      <td>[23209, 23245, 23298, 47566, 22993, 22328, 223...</td>\n",
       "      <td>20727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4474</th>\n",
       "      <td>503616</td>\n",
       "      <td>15215</td>\n",
       "      <td>[48185, 48138, 48184, 22499, 22138, 21559, 217...</td>\n",
       "      <td>[door|mat|fairy|cake, door|mat|union|flag, doo...</td>\n",
       "      <td>[7.49, 7.49, 7.49, 5.95, 4.95, 2.55, 5.95, 3.9...</td>\n",
       "      <td>10</td>\n",
       "      <td>[48185, 48138, 48184, 22499, 22138, 21559, 217...</td>\n",
       "      <td>21915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17051</th>\n",
       "      <td>539374</td>\n",
       "      <td>14769</td>\n",
       "      <td>[82486, 82484, 82483, 71459, 84755, 22722, 227...</td>\n",
       "      <td>[wood|s/3|cabinet|ant|white|finish, wood|black...</td>\n",
       "      <td>[7.95, 6.45, 5.95, 0.85, 0.65, 3.95, 4.95, 4.9...</td>\n",
       "      <td>16</td>\n",
       "      <td>[82486, 82484, 82483, 71459, 84755, 22722, 227...</td>\n",
       "      <td>22077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Invoice  Customer ID                                          StockCode  \\\n",
       "24230  559989        13319  [23209, 23245, 23298, 47566, 22993, 22328, 223...   \n",
       "4474   503616        15215  [48185, 48138, 48184, 22499, 22138, 21559, 217...   \n",
       "17051  539374        14769  [82486, 82484, 82483, 71459, 84755, 22722, 227...   \n",
       "\n",
       "                                             Description  \\\n",
       "24230  [lunch|bag|doiley|pattern, set|of|3|regency|ca...   \n",
       "4474   [door|mat|fairy|cake, door|mat|union|flag, doo...   \n",
       "17051  [wood|s/3|cabinet|ant|white|finish, wood|black...   \n",
       "\n",
       "                                                   Price  Num products  \\\n",
       "24230  [1.65, 4.95, 4.95, 4.95, 1.25, 2.95, 2.95, 1.2...            10   \n",
       "4474   [7.49, 7.49, 7.49, 5.95, 4.95, 2.55, 5.95, 3.9...            10   \n",
       "17051  [7.95, 6.45, 5.95, 0.85, 0.65, 3.95, 4.95, 4.9...            16   \n",
       "\n",
       "                                                       X      Y  \n",
       "24230  [23209, 23245, 23298, 47566, 22993, 22328, 223...  20727  \n",
       "4474   [48185, 48138, 48184, 22499, 22138, 21559, 217...  21915  \n",
       "17051  [82486, 82484, 82483, 71459, 84755, 22722, 227...  22077  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Keep only relevent features & baskets with some items -> Single item purchases are targeted and are therefore irrelevant\n",
    "Carts = baskets[ baskets['Num products']>= 5].drop(columns=['Purchase Date', 'Purchase Time', 'InvoiceDate', 'Quantity', 'Amount', 'Total amount'])\n",
    "\n",
    "# we assume all purchases will be made today -> recency bias will not be trained well.\n",
    "Carts['X'] = Carts['StockCode'].apply(lambda x: x[:-1])\n",
    "Carts['Y'] = Carts['StockCode'].apply(lambda x: x[-1])\n",
    "print(\"Carts shape: \",Carts.shape)\n",
    "Carts.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2bc4f8",
   "metadata": {},
   "source": [
    "#### 2) Function to calculate loss for each parameters mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4299abec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 13.324175564870082\n"
     ]
    }
   ],
   "source": [
    "def Categorical_Cross_Entropy(params, model, carts, top_n=10, batch_size=128):\n",
    "    \"\"\"\n",
    "    Computes categorical cross-entropy for a single true item and predicted probabilities.\n",
    "\n",
    "    params : list or np.array\n",
    "        Model coefficients [alpha, beta, gamma, delta, epsilon, eta, d_effect]\n",
    "    model : Retail_Recommendation_optimal_desc\n",
    "    carts : pd.DataFrame\n",
    "        Must have columns ['Customer ID', 'X', 'Y']\n",
    "    top_n : int\n",
    "        Number of top items to consider from recommendations\n",
    "    batch_size : int\n",
    "        Number of baskets to process in one batch  \n",
    "        \n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    valid_count = 0\n",
    "    \n",
    "    model.weights = params # load current weights\n",
    "    n_batches = int(np.ceil(len(carts) / batch_size))\n",
    "    \n",
    "    correct = 0\n",
    "    for b in range(n_batches):\n",
    "        batch = carts.iloc[b*batch_size:(b+1)*batch_size]\n",
    "\n",
    "        X_batch = batch.drop(columns=['Y']) # basket\n",
    "\n",
    "        # Generate recommendations (returns DataFrame)\n",
    "        result = model.recommend(X_batch, top_n=top_n)  # choose sufficient top_n\n",
    "        \n",
    "        # Check if target item is in top-N\n",
    "        for idx,(basket_idx, basket) in enumerate(batch.iterrows()):\n",
    "            Y = basket['Y']\n",
    "            rec_df = list(result.values())[idx]\n",
    "            \n",
    "            if Y in set(rec_df['StockCode']):\n",
    "                prob = rec_df.loc[rec_df['StockCode'] == Y, 'Probability'].values[0]\n",
    "                total_loss += -np.log(prob)\n",
    "                correct += 1\n",
    "            else:\n",
    "                total_loss += -np.log(1e-6)\n",
    "                # display(items.loc[items['StockCode'] == Y])\n",
    "\n",
    "            valid_count += 1\n",
    "        \"\"\" If the model didn’t predict the correct item, we assign it a very low probability penalty, meaning huge loss\"\"\"\n",
    "    \n",
    "    # print('Correct: ',correct,\" out of: \",valid_count)\n",
    "    # represents -> average loss\n",
    "    return (correct ,(total_loss / valid_count if valid_count > 0 else np.nan))\n",
    "    \n",
    "    ## for single item -> y_true = 1\n",
    "## return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "loss = Categorical_Cross_Entropy(params, Model, Carts.sample(100))\n",
    "print(\"Initial loss:\", loss[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42ead3",
   "metadata": {},
   "source": [
    "#### 3) Function to train model:\n",
    "We will run thorugh a sample of baskets to calculate loss. Then we will update the weights using numerical gradient descent.\n",
    "For calculating numerical gradient of our loss function, we have two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd8594f",
   "metadata": {},
   "source": [
    "##### **1) Perturbation effect:**\n",
    "Perturbation effect is a numerical method to estimate the gradient of a loss function by slightly changing (perturbing) each parameter and observing the change in loss. It works because the gradient represents the rate of change of the loss with respect to the parameter; by measuring how the loss changes with a small parameter shift, we approximate this rate. This is simple to implement but slower and less precise than analytical gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab24cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_model(model, Carts, lr=0.05, n_iter=20, error=1e-4, sample_size=100,batch_size=32, top_n=10):\n",
    "    \"\"\"\n",
    "    Train the Retail_Recommendation model by optimizing weights and d_effect\n",
    "    using numerical gradient descent on categorical cross-entropy loss.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Retail_Recommendation\n",
    "        Your initialized model\n",
    "    carts : pd.DataFrame\n",
    "        Must have columns ['Customer ID', 'X', 'Y']\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    n_iter : int\n",
    "        Number of gradient descent iterations\n",
    "    error : float\n",
    "        Small perturbation for numerical gradient\n",
    "    sample_size : int\n",
    "        Numbers of baskets to consider for each iteration\n",
    "    batch_size : int\n",
    "        Number of baskets to process per batch for efficiency\n",
    "    top_n : int\n",
    "        How many items to consider for loss evaluation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_params : dict\n",
    "        Optimized model parameters\n",
    "    history : list\n",
    "        Loss per epoch\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    params = copy.deepcopy(model.weights) # completely new variables\n",
    "    params['d_effect'] = model.d_effect\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(1, n_iter+1):\n",
    "        # ---- sample once per epoch for stable gradient estimation ----\n",
    "        sample = Carts.sample(sample_size, random_state=epoch)\n",
    "        \n",
    "        # compute numerical gradient\n",
    "        grads = {}\n",
    "        for key in params.keys():\n",
    "            orig = params[key]\n",
    "            \n",
    "            # perturn +error\n",
    "            params[key] = orig + error\n",
    "            model.history_cache.clear()\n",
    "            model.recency_cache.clear()\n",
    "            _, loss_plus = Categorical_Cross_Entropy(params, model, sample, top_n=top_n, batch_size=batch_size)\n",
    "            # perturn -error\n",
    "            params[key] = orig - error\n",
    "            model.history_cache.clear()\n",
    "            model.recency_cache.clear()\n",
    "            _, loss_minus = Categorical_Cross_Entropy(params, model, sample, top_n=top_n, batch_size=batch_size)\n",
    "            \n",
    "            # compute central difference gradient\n",
    "            grads[key] = (loss_plus - loss_minus) / (2 * error)\n",
    "            # restore params\n",
    "            params[key] = orig\n",
    "        \n",
    "        # update weights\n",
    "        for key in params.keys():\n",
    "            params[key] -= lr * grads[key]\n",
    "            \n",
    "        model.history_cache.clear()\n",
    "        model.recency_cache.clear()\n",
    "        # evaluate updated params once on the same sample to get loss and correct_count\n",
    "        correct_total, epoch_loss = Categorical_Cross_Entropy(params, model, sample, top_n=top_n, batch_size=batch_size)\n",
    "        \n",
    "        # store history\n",
    "        history.append( (epoch_loss, int(correct_total), copy.deepcopy(params)) )\n",
    "        print(f\"Epoch {epoch+1}/{n_iter} - Loss: {epoch_loss:.5f}, Correct: {correct_total}/{sample_size})\")\n",
    "  \n",
    "    \n",
    "    # Update model with final weights\n",
    "    model.weights = {k: v for k, v in params.items() if k != 'd_effect'}\n",
    "    model.d_effect = params['d_effect']\n",
    "    \n",
    "    return copy.deepcopy(params), history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32fee56",
   "metadata": {},
   "source": [
    "##### **2) Analytical effect**:\n",
    "Analytical gradient method computes the exact gradient of the loss function with respect to each model parameter directly from the probability formula. It works by differentiating the loss function algebraically, which provides precise gradient values for optimization. This approach is faster and more accurate than perturbation, especially for models with many parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b473bb",
   "metadata": {},
   "source": [
    "For each basket, the logit for item `i` is:\n",
    "$$ log(Z_{u,i}) = b_i + \\alpha*H_i + \\beta*R_i + \\delta*P_i + \\eta*D_i + \\gamma*T_i + \\epsilon*log(1 + C_i) $$\n",
    "$$ P_{u,i} = softmax( log(Z_{u,i}) )  $$\n",
    "\n",
    "Then loss for single target item `y` is:\n",
    "$$ loss = -log(P_{u,y}) $$\n",
    "\n",
    "Using standard softmax cross-entropy derivatives:\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\sum_i (p_i - y_i).f^w_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecef9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def train_model_analytical(model, carts, top_n=25, batch_size=128, lr=0.05, n_epochs=20):\n",
    "    \"\"\"\n",
    "    Analytical gradient descent training for Retail_Recommendation.\n",
    "    Computes gradient from softmax probabilities and updates model weights.\n",
    "    \"\"\"\n",
    "    # Prepare weights and initialize history\n",
    "    params = copy.deepcopy(model.weights)\n",
    "    params['d_effect'] = model.d_effect\n",
    "    model.weights = params\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        total_loss, total_correct = 0.0, 0\n",
    "        grads = {k: 0.0 for k in model.weights.keys()}\n",
    "\n",
    "        n_batches = int(np.ceil(len(carts) / batch_size))\n",
    "\n",
    "        for b in range(n_batches):\n",
    "            batch = carts.iloc[b*batch_size : (b+1)*batch_size]\n",
    "            X_batch = batch.drop(columns=['Y'])\n",
    "            Y_batch = batch['Y'].values\n",
    "\n",
    "            # Run recommendations\n",
    "            result = model.recommend(X_batch, top_n=top_n)\n",
    "\n",
    "            for idx, (basket_idx, row) in enumerate(batch.iterrows()):\n",
    "                cid = row['Customer ID']\n",
    "                basket_items = row['X']\n",
    "                Y = Y_batch[idx]\n",
    "                rec_df = list(result.values())[idx]\n",
    "\n",
    "                # If true item not found — assign small probability\n",
    "                if Y not in rec_df['StockCode'].values:\n",
    "                    prob = 1e-6\n",
    "                    total_loss += -np.log(prob)\n",
    "                    continue\n",
    "\n",
    "                # Probability for the true item\n",
    "                prob = rec_df.loc[rec_df['StockCode'] == Y, 'Probability'].values[0]\n",
    "                total_loss += -np.log(prob)\n",
    "                total_correct += 1\n",
    "\n",
    "                # ---- Gradient computation (analytical form) ----\n",
    "                # Recompute logit components for this basket\n",
    "                H = model.compute_history(cid)\n",
    "                R = model.compute_rules_array(basket_items)\n",
    "                T = model.compute_recency(cid)\n",
    "                P = model.compute_price_array(None)\n",
    "                D = model.compute_discount_array(None)\n",
    "\n",
    "                # Feature vector (partial derivatives of logit w.r.t weights)\n",
    "                features = {\n",
    "                    'alpha': H,\n",
    "                    'beta': R,\n",
    "                    'gamma': T,\n",
    "                    'delta': P,\n",
    "                    'eta': D\n",
    "                }\n",
    "\n",
    "                # Predicted probabilities over all items (softmax)\n",
    "                logit = (model.Bias\n",
    "                         + params['alpha'] * H\n",
    "                         + params['beta'] * R\n",
    "                         + params['gamma'] * T\n",
    "                         + params['delta'] * P\n",
    "                         + params['eta'] * D)\n",
    "                exps = np.exp(logit - logit.max())\n",
    "                probs = exps / (exps.sum() + 1e-9)\n",
    "\n",
    "                # Gradient for each weight\n",
    "                for key in features.keys():\n",
    "                    grads[key] += np.sum((probs - (model.all_items == Y)) * features[key])\n",
    "\n",
    "        # Average gradients\n",
    "        for key in grads.keys():\n",
    "            grads[key] /= len(carts)\n",
    "\n",
    "        # Gradient descent update\n",
    "        for key in grads.keys():\n",
    "            params[key] -= lr * grads[key]\n",
    "\n",
    "        # Save model weights after epoch\n",
    "        model.weights = {k: v for k, v in params.items() if k != 'd_effect'}\n",
    "        model.d_effect = params['d_effect']\n",
    "\n",
    "        avg_loss = total_loss / len(carts)\n",
    "        history.append((avg_loss, total_correct, copy.deepcopy(params)))\n",
    "        print(f\"Epoch {epoch}/{n_epochs} - Loss: {avg_loss:.5f}, Correct: {total_correct}/{len(carts)}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe946144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def train_model_analytically(model, carts, top_n=25, batch_size=128, lr=0.1, n_epochs=20):\n",
    "    def compute_features(baskets_df):\n",
    "        n = len(baskets_df)\n",
    "        H = np.zeros((n, model.n_items), dtype=np.float32)\n",
    "        R = np.zeros((n, model.n_items), dtype=np.float32)\n",
    "        T = np.zeros((n, model.n_items), dtype=np.float32)\n",
    "        P = np.zeros((n, model.n_items), dtype=np.float32)\n",
    "        D = np.zeros((n, model.n_items), dtype=np.float32)\n",
    "\n",
    "        for idx, (_,row) in enumerate(baskets_df.iterrows()):\n",
    "            cid = row['Customer ID']\n",
    "            basket_items = row['X'] if isinstance(row['X'], (list, set)) else []\n",
    "            \n",
    "            H[idx] = model.compute_history(cid)\n",
    "            T[idx] = model.compute_recency(cid)\n",
    "            R[idx] = model.compute_rules_array(basket_items)\n",
    "            P[idx] = model.compute_price_array(None)\n",
    "            D[idx] = model.compute_discount_array(None)\n",
    "        return {'H': H, 'R': R, 'T': T, 'P': P, 'D': D}\n",
    "\n",
    "    # Initialize parameters\n",
    "    params = copy.deepcopy(model.weights)\n",
    "    params['d_effect'] = model.d_effect\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss, correct_total, total_count = 0.0, 0, 0\n",
    "        grads = {k: 0.0 for k in params.keys()}\n",
    "\n",
    "        n_batches = int(np.ceil(len(carts) / batch_size))\n",
    "        for b in range(n_batches):\n",
    "            batch = carts.iloc[b*batch_size:(b+1)*batch_size]\n",
    "            X_batch = batch.drop(columns=['Y'])\n",
    "            Y_batch = batch['Y'].values\n",
    "\n",
    "            feats = compute_features(X_batch)\n",
    "\n",
    "            logit = (model.Bias\n",
    "                     + params['alpha'] * feats['H']\n",
    "                     + params['beta']  * feats['R']\n",
    "                     + params['gamma'] * feats['T']\n",
    "                     + params['delta'] * feats['P']\n",
    "                     + params['eta']   * feats['D'])\n",
    "            logit -= logit.max(axis=1, keepdims=True)\n",
    "            exps = np.exp(logit)\n",
    "            probs = exps / (exps.sum(axis=1, keepdims=True) + 1e-9)\n",
    "\n",
    "            # Map Y to index in all_items\n",
    "            Y_idx = np.array([np.where(model.all_items == y)[0][0] if y in model.all_items else -1 for y in Y_batch])\n",
    "            valid_mask = (Y_idx != -1)\n",
    "            batch_loss = -np.log(probs[np.arange(len(Y_batch))[valid_mask], Y_idx[valid_mask]] + 1e-9).mean()\n",
    "            total_loss += batch_loss * len(Y_batch)\n",
    "\n",
    "            one_hot = np.zeros_like(probs)\n",
    "            one_hot[np.arange(len(Y_batch))[valid_mask], Y_idx[valid_mask]] = 1\n",
    "            diff = (probs - one_hot) / len(Y_batch)\n",
    "\n",
    "            grads['alpha'] += np.sum(diff * feats['H'])\n",
    "            grads['beta']  += np.sum(diff * feats['R'])\n",
    "            grads['gamma'] += np.sum(diff * feats['T'])\n",
    "            grads['delta'] += np.sum(diff * feats['P'])\n",
    "            grads['eta']   += np.sum(diff * feats['D'])\n",
    "\n",
    "            # Accuracy\n",
    "            top_preds = np.argmax(probs, axis=1)\n",
    "            correct_total += np.sum((model.all_items[top_preds] == Y_batch) & valid_mask)\n",
    "            total_count += np.sum(valid_mask)\n",
    "\n",
    "        # Update weights\n",
    "        for k in grads.keys():\n",
    "            grads[k] /= total_count\n",
    "            params[k] -= lr * grads[k]\n",
    "\n",
    "        model.weights = {k: v for k, v in params.items() if k != 'd_effect'}\n",
    "        model.d_effect = params['d_effect']\n",
    "\n",
    "        epoch_loss = total_loss / total_count\n",
    "        epoch_acc = correct_total / total_count\n",
    "        history.append((epoch_loss, epoch_acc, copy.deepcopy(params)))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Loss: {epoch_loss:.5f} | Accuracy: {epoch_acc:.3f}\")\n",
    "        print(\"Weights:\", {k: round(v, 4) for k, v in params.items()})\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9031aadf",
   "metadata": {},
   "source": [
    "#### Line-by-line explanation\n",
    "\n",
    "1. Y_idx = np.array([ ... ])\n",
    "- Maps each true label in Y_batch (which are item codes) to an integer index in `self.all_items`.\n",
    "- If a true label is not found in `self.all_items`, it stores -1 (meaning invalid / unknown target).\n",
    "\n",
    "2. valid_mask = Y_idx != -1\n",
    "- A boolean array marking which rows have a valid (known) target. We will only compute loss/gradients for those rows.\n",
    "\n",
    "3. if np.any(valid_mask):\n",
    "- Only proceed if at least one target in the batch is valid.\n",
    "\n",
    "4. batch_loss = -np.log(probs[np.arange(len(Y_batch))[valid_mask], Y_idx[valid_mask]] + 1e-9).mean()\n",
    "- probs has shape `[batch_size, n_items]` and contains the predicted probability for each item for each basket.\n",
    "- `np.arange(len(Y_batch))[valid_mask]` picks the row indices (samples) that have valid targets.\n",
    "- `Y_idx[valid_mask]` picks the target class index for each of those valid samples.\n",
    "- `probs[rows, cols]` extracts the predicted probability assigned to the true class for each valid sample.\n",
    "- `-np.log(...)` computes negative log-likelihood for each valid sample.\n",
    "- `.mean()` takes the mean across the valid samples in the batch → this is the average cross-entropy for that batch.\n",
    "\n",
    "5. one_hot = np.zeros_like(probs)\n",
    "- Creates an all-zeros array same shape as probs to build one-hot encodings of the true class per row.\n",
    "\n",
    "6. one_hot[np.arange(len(Y_batch))[valid_mask], Y_idx[valid_mask]] = 1\n",
    "- Places ones in one_hot at the (row, true_class) positions — only for valid rows.\n",
    "\n",
    "7. diff = (probs - one_hot) / len(Y_batch)\n",
    "- This is the (vectorized) gradient of the average cross-entropy loss with respect to the logits after softmax.\n",
    "- For softmax + cross-entropy, ∂L/∂z = (probs − one_hot), where z are logits. Dividing by `len(Y_batch)` scales the gradient by batch size (makes the gradient the per-sample average when later summed).\n",
    "\n",
    "##### Note: \n",
    "the code divides by `len(Y_batch)` (batch size), not by `valid_count`. This scales gradients consistently across batches, but if a lot of targets are invalid you might prefer dividing by `valid_count` — both are reasonable choices but they change effective step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5039d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_result = train_model(Model, Carts, lr=0.1, n_iter=5, error=1e-4, sample_size=100, batch_size=32, top_n=25)\n",
    "# final_weight, hist = trained_result\n",
    "# print(\"History: \")\n",
    "# for i, (loss, corrent, parameter) in enumerate(hist):\n",
    "#     print(f\"For {i} epoch, loss: {loss:.5f},  correct: {corrent}, parameter: {parameter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b61897",
   "metadata": {},
   "source": [
    "Out of these two unique methods, we will use analytical gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1758c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model = train_model_analytically(Model, Carts.sample(1000), top_n=10, batch_size=128, lr=0.05, n_epochs=100)\n",
    "# final_weight = trained_model[0].weights\n",
    "# print(\"Final Weights: \")\n",
    "# final_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa776f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# History = trained_model[1]\n",
    "# print(\"History of model: \",)\n",
    "# for iter in History:\n",
    "#     print(iter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "970da6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # Store best params\n",
    "# with open('../Models/best_parameters.pkl', 'wb') as f:\n",
    "#     pickle.dump(final_weight, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba7171",
   "metadata": {},
   "source": [
    "We will add these two function into our `Model.py` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3fe713",
   "metadata": {},
   "source": [
    "For training price array, we need some sort of pseudo budget parameter for each basket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b232c7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 40/40 [00:03<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Loss: 7.521313 \n",
      "params: {'alpha': np.float32(0.9951), 'beta': np.float32(1.2451), 'delta': np.float32(1.0001), 'eta': np.float32(0.9951), 'gamma': np.float32(0.0995), 'epsilon': np.float32(2.2451), 'd_effect': np.float32(0.4975)}\n",
      "Correct predictions:  4748/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 40/40 [00:03<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 | Loss: 7.324176 \n",
      "params: {'alpha': np.float32(0.9903), 'beta': np.float32(1.4855), 'delta': np.float32(1.0006), 'eta': np.float32(0.9903), 'gamma': np.float32(0.099), 'epsilon': np.float32(2.4855), 'd_effect': np.float32(0.4952)}\n",
      "Correct predictions:  4789/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 40/40 [00:03<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 | Loss: 7.110485 \n",
      "params: {'alpha': np.float32(0.9856), 'beta': np.float32(1.7213), 'delta': np.float32(1.002), 'eta': np.float32(0.9856), 'gamma': np.float32(0.0986), 'epsilon': np.float32(2.7213), 'd_effect': np.float32(0.4928)}\n",
      "Correct predictions:  4814/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 40/40 [00:03<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 | Loss: 6.955095 \n",
      "params: {'alpha': np.float32(0.9811), 'beta': np.float32(1.9528), 'delta': np.float32(1.0016), 'eta': np.float32(0.9811), 'gamma': np.float32(0.0981), 'epsilon': np.float32(2.9528), 'd_effect': np.float32(0.4905)}\n",
      "Correct predictions:  4833/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 40/40 [00:03<00:00, 10.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 | Loss: 6.787812 \n",
      "params: {'alpha': np.float32(0.9766), 'beta': np.float32(2.1801), 'delta': np.float32(1.0024), 'eta': np.float32(0.9766), 'gamma': np.float32(0.0977), 'epsilon': np.float32(3.1801), 'd_effect': np.float32(0.4883)}\n",
      "Correct predictions:  4837/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 40/40 [00:04<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 | Loss: 6.640415 \n",
      "params: {'alpha': np.float32(0.9723), 'beta': np.float32(2.4033), 'delta': np.float32(1.0032), 'eta': np.float32(0.9723), 'gamma': np.float32(0.0972), 'epsilon': np.float32(3.4033), 'd_effect': np.float32(0.4861)}\n",
      "Correct predictions:  4836/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 40/40 [00:03<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 | Loss: 6.531035 \n",
      "params: {'alpha': np.float32(0.968), 'beta': np.float32(2.6226), 'delta': np.float32(1.0032), 'eta': np.float32(0.968), 'gamma': np.float32(0.0968), 'epsilon': np.float32(3.6226), 'd_effect': np.float32(0.484)}\n",
      "Correct predictions:  4817/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 40/40 [00:04<00:00,  9.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 | Loss: 6.392059 \n",
      "params: {'alpha': np.float32(0.9638), 'beta': np.float32(2.8381), 'delta': np.float32(1.0023), 'eta': np.float32(0.9638), 'gamma': np.float32(0.0964), 'epsilon': np.float32(3.8381), 'd_effect': np.float32(0.4819)}\n",
      "Correct predictions:  4820/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 40/40 [00:04<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 | Loss: 6.261491 \n",
      "params: {'alpha': np.float32(0.9597), 'beta': np.float32(3.0156), 'delta': np.float32(1.0015), 'eta': np.float32(0.9597), 'gamma': np.float32(0.096), 'epsilon': np.float32(4.05), 'd_effect': np.float32(0.4799)}\n",
      "Correct predictions:  4843/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 40/40 [00:03<00:00, 10.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 | Loss: 6.118893 \n",
      "params: {'alpha': np.float32(0.9557), 'beta': np.float32(3.1794), 'delta': np.float32(0.9996), 'eta': np.float32(0.9557), 'gamma': np.float32(0.0956), 'epsilon': np.float32(4.2583), 'd_effect': np.float32(0.4779)}\n",
      "Correct predictions:  4843/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 40/40 [00:03<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 | Loss: 6.043582 \n",
      "params: {'alpha': np.float32(0.9518), 'beta': np.float32(3.3032), 'delta': np.float32(0.9978), 'eta': np.float32(0.9518), 'gamma': np.float32(0.0952), 'epsilon': np.float32(4.4632), 'd_effect': np.float32(0.4759)}\n",
      "Correct predictions:  4823/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 40/40 [00:03<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 | Loss: 5.912457 \n",
      "params: {'alpha': np.float32(0.948), 'beta': np.float32(3.3808), 'delta': np.float32(0.9968), 'eta': np.float32(0.948), 'gamma': np.float32(0.0948), 'epsilon': np.float32(4.6648), 'd_effect': np.float32(0.474)}\n",
      "Correct predictions:  4864/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 40/40 [00:04<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 | Loss: 5.894025 \n",
      "params: {'alpha': np.float32(0.9442), 'beta': np.float32(3.4222), 'delta': np.float32(0.9949), 'eta': np.float32(0.9442), 'gamma': np.float32(0.0944), 'epsilon': np.float32(4.8633), 'd_effect': np.float32(0.4721)}\n",
      "Correct predictions:  4820/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 40/40 [00:03<00:00, 10.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 | Loss: 5.768508 \n",
      "params: {'alpha': np.float32(0.9405), 'beta': np.float32(3.4355), 'delta': np.float32(0.9924), 'eta': np.float32(0.9405), 'gamma': np.float32(0.0941), 'epsilon': np.float32(5.0586), 'd_effect': np.float32(0.4703)}\n",
      "Correct predictions:  4852/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 40/40 [00:04<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 | Loss: 5.719904 \n",
      "params: {'alpha': np.float32(0.9369), 'beta': np.float32(3.4328), 'delta': np.float32(0.9896), 'eta': np.float32(0.9369), 'gamma': np.float32(0.0937), 'epsilon': np.float32(5.2509), 'd_effect': np.float32(0.4685)}\n",
      "Correct predictions:  4845/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 40/40 [00:03<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 | Loss: 5.658514 \n",
      "params: {'alpha': np.float32(0.9334), 'beta': np.float32(3.4255), 'delta': np.float32(0.9881), 'eta': np.float32(0.9334), 'gamma': np.float32(0.0933), 'epsilon': np.float32(5.4403), 'd_effect': np.float32(0.4667)}\n",
      "Correct predictions:  4837/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 40/40 [00:04<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 | Loss: 5.570711 \n",
      "params: {'alpha': np.float32(0.9299), 'beta': np.float32(3.3864), 'delta': np.float32(0.9856), 'eta': np.float32(0.9299), 'gamma': np.float32(0.093), 'epsilon': np.float32(5.6268), 'd_effect': np.float32(0.4649)}\n",
      "Correct predictions:  4846/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 40/40 [00:04<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 | Loss: 5.518690 \n",
      "params: {'alpha': np.float32(0.9265), 'beta': np.float32(3.3286), 'delta': np.float32(0.9832), 'eta': np.float32(0.9265), 'gamma': np.float32(0.0926), 'epsilon': np.float32(5.8107), 'd_effect': np.float32(0.4632)}\n",
      "Correct predictions:  4856/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 40/40 [00:04<00:00,  9.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 | Loss: 5.428168 \n",
      "params: {'alpha': np.float32(0.9231), 'beta': np.float32(3.273), 'delta': np.float32(0.9806), 'eta': np.float32(0.9231), 'gamma': np.float32(0.0923), 'epsilon': np.float32(5.9918), 'd_effect': np.float32(0.4616)}\n",
      "Correct predictions:  4869/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 40/40 [00:04<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 | Loss: 5.372914 \n",
      "params: {'alpha': np.float32(0.9198), 'beta': np.float32(3.2102), 'delta': np.float32(0.9778), 'eta': np.float32(0.9198), 'gamma': np.float32(0.092), 'epsilon': np.float32(6.1704), 'd_effect': np.float32(0.4599)}\n",
      "Correct predictions:  4850/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 40/40 [00:03<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 | Loss: 5.308037 \n",
      "params: {'alpha': np.float32(0.9166), 'beta': np.float32(3.1312), 'delta': np.float32(0.9754), 'eta': np.float32(0.9166), 'gamma': np.float32(0.0917), 'epsilon': np.float32(6.3464), 'd_effect': np.float32(0.4583)}\n",
      "Correct predictions:  4862/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 40/40 [00:03<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 | Loss: 5.267775 \n",
      "params: {'alpha': np.float32(0.9134), 'beta': np.float32(3.0559), 'delta': np.float32(0.9736), 'eta': np.float32(0.9134), 'gamma': np.float32(0.0913), 'epsilon': np.float32(6.5201), 'd_effect': np.float32(0.4567)}\n",
      "Correct predictions:  4860/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 40/40 [00:03<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 | Loss: 5.216151 \n",
      "params: {'alpha': np.float32(0.9103), 'beta': np.float32(2.9773), 'delta': np.float32(0.9712), 'eta': np.float32(0.9103), 'gamma': np.float32(0.091), 'epsilon': np.float32(6.6913), 'd_effect': np.float32(0.4551)}\n",
      "Correct predictions:  4865/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 40/40 [00:03<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 | Loss: 5.182287 \n",
      "params: {'alpha': np.float32(0.9072), 'beta': np.float32(2.9017), 'delta': np.float32(0.9694), 'eta': np.float32(0.9072), 'gamma': np.float32(0.0907), 'epsilon': np.float32(6.8602), 'd_effect': np.float32(0.4536)}\n",
      "Correct predictions:  4847/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 40/40 [00:03<00:00, 11.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 | Loss: 5.124405 \n",
      "params: {'alpha': np.float32(0.9042), 'beta': np.float32(2.8207), 'delta': np.float32(0.9667), 'eta': np.float32(0.9042), 'gamma': np.float32(0.0904), 'epsilon': np.float32(7.0269), 'd_effect': np.float32(0.4521)}\n",
      "Correct predictions:  4856/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 40/40 [00:03<00:00, 10.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 | Loss: 5.088299 \n",
      "params: {'alpha': np.float32(0.9012), 'beta': np.float32(2.7409), 'delta': np.float32(0.9646), 'eta': np.float32(0.9012), 'gamma': np.float32(0.0901), 'epsilon': np.float32(7.1914), 'd_effect': np.float32(0.4506)}\n",
      "Correct predictions:  4845/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 40/40 [00:03<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 | Loss: 5.047820 \n",
      "params: {'alpha': np.float32(0.8983), 'beta': np.float32(2.6682), 'delta': np.float32(0.9624), 'eta': np.float32(0.8983), 'gamma': np.float32(0.0898), 'epsilon': np.float32(7.3537), 'd_effect': np.float32(0.4491)}\n",
      "Correct predictions:  4841/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|██████████| 40/40 [00:03<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 | Loss: 4.967681 \n",
      "params: {'alpha': np.float32(0.8954), 'beta': np.float32(2.5955), 'delta': np.float32(0.9607), 'eta': np.float32(0.8954), 'gamma': np.float32(0.0895), 'epsilon': np.float32(7.5139), 'd_effect': np.float32(0.4477)}\n",
      "Correct predictions:  4856/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|██████████| 40/40 [00:03<00:00, 11.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 | Loss: 4.949382 \n",
      "params: {'alpha': np.float32(0.8926), 'beta': np.float32(2.5108), 'delta': np.float32(0.9589), 'eta': np.float32(0.8926), 'gamma': np.float32(0.0893), 'epsilon': np.float32(7.6722), 'd_effect': np.float32(0.4463)}\n",
      "Correct predictions:  4850/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|██████████| 40/40 [00:03<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 | Loss: 4.901289 \n",
      "params: {'alpha': np.float32(0.8898), 'beta': np.float32(2.4222), 'delta': np.float32(0.9569), 'eta': np.float32(0.8898), 'gamma': np.float32(0.089), 'epsilon': np.float32(7.8284), 'd_effect': np.float32(0.4449)}\n",
      "Correct predictions:  4863/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|██████████| 40/40 [00:03<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 | Loss: 4.828310 \n",
      "params: {'alpha': np.float32(0.887), 'beta': np.float32(2.3524), 'delta': np.float32(0.9547), 'eta': np.float32(0.887), 'gamma': np.float32(0.0887), 'epsilon': np.float32(7.9827), 'd_effect': np.float32(0.4435)}\n",
      "Correct predictions:  4853/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|██████████| 40/40 [00:03<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 | Loss: 4.787457 \n",
      "params: {'alpha': np.float32(0.8843), 'beta': np.float32(2.2784), 'delta': np.float32(0.9532), 'eta': np.float32(0.8843), 'gamma': np.float32(0.0884), 'epsilon': np.float32(8.1352), 'd_effect': np.float32(0.4422)}\n",
      "Correct predictions:  4864/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|██████████| 40/40 [00:03<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 | Loss: 4.801465 \n",
      "params: {'alpha': np.float32(0.8816), 'beta': np.float32(2.1948), 'delta': np.float32(0.9516), 'eta': np.float32(0.8816), 'gamma': np.float32(0.0882), 'epsilon': np.float32(8.2858), 'd_effect': np.float32(0.4408)}\n",
      "Correct predictions:  4855/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|██████████| 40/40 [00:03<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 | Loss: 4.751804 \n",
      "params: {'alpha': np.float32(0.879), 'beta': np.float32(2.1202), 'delta': np.float32(0.9501), 'eta': np.float32(0.879), 'gamma': np.float32(0.0879), 'epsilon': np.float32(8.4346), 'd_effect': np.float32(0.4395)}\n",
      "Correct predictions:  4850/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|██████████| 40/40 [00:03<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 | Loss: 4.705212 \n",
      "params: {'alpha': np.float32(0.8764), 'beta': np.float32(2.0355), 'delta': np.float32(0.9481), 'eta': np.float32(0.8764), 'gamma': np.float32(0.0876), 'epsilon': np.float32(8.5817), 'd_effect': np.float32(0.4382)}\n",
      "Correct predictions:  4866/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|██████████| 40/40 [00:03<00:00, 10.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 | Loss: 4.652225 \n",
      "params: {'alpha': np.float32(0.8739), 'beta': np.float32(1.9728), 'delta': np.float32(0.9468), 'eta': np.float32(0.8739), 'gamma': np.float32(0.0874), 'epsilon': np.float32(8.727), 'd_effect': np.float32(0.4369)}\n",
      "Correct predictions:  4853/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|██████████| 40/40 [00:03<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 | Loss: 4.645550 \n",
      "params: {'alpha': np.float32(0.8714), 'beta': np.float32(1.8971), 'delta': np.float32(0.9458), 'eta': np.float32(0.8714), 'gamma': np.float32(0.0871), 'epsilon': np.float32(8.8707), 'd_effect': np.float32(0.4357)}\n",
      "Correct predictions:  4860/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|██████████| 40/40 [00:03<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 | Loss: 4.617890 \n",
      "params: {'alpha': np.float32(0.8689), 'beta': np.float32(1.8095), 'delta': np.float32(0.9441), 'eta': np.float32(0.8689), 'gamma': np.float32(0.0869), 'epsilon': np.float32(9.0127), 'd_effect': np.float32(0.4345)}\n",
      "Correct predictions:  4851/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|██████████| 40/40 [00:04<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 | Loss: 4.524046 \n",
      "params: {'alpha': np.float32(0.8665), 'beta': np.float32(1.7394), 'delta': np.float32(0.9422), 'eta': np.float32(0.8665), 'gamma': np.float32(0.0866), 'epsilon': np.float32(9.1532), 'd_effect': np.float32(0.4332)}\n",
      "Correct predictions:  4871/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|██████████| 40/40 [00:04<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 | Loss: 4.553234 \n",
      "params: {'alpha': np.float32(0.8641), 'beta': np.float32(1.6603), 'delta': np.float32(0.9409), 'eta': np.float32(0.8641), 'gamma': np.float32(0.0864), 'epsilon': np.float32(9.2921), 'd_effect': np.float32(0.432)}\n",
      "Correct predictions:  4855/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|██████████| 40/40 [00:04<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 | Loss: 4.538695 \n",
      "params: {'alpha': np.float32(0.8617), 'beta': np.float32(1.5994), 'delta': np.float32(0.9396), 'eta': np.float32(0.8617), 'gamma': np.float32(0.0862), 'epsilon': np.float32(9.4294), 'd_effect': np.float32(0.4308)}\n",
      "Correct predictions:  4843/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|██████████| 40/40 [00:03<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 | Loss: 4.464477 \n",
      "params: {'alpha': np.float32(0.8593), 'beta': np.float32(1.5229), 'delta': np.float32(0.9388), 'eta': np.float32(0.8593), 'gamma': np.float32(0.0859), 'epsilon': np.float32(9.5653), 'd_effect': np.float32(0.4297)}\n",
      "Correct predictions:  4869/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|██████████| 40/40 [00:04<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 | Loss: 4.444019 \n",
      "params: {'alpha': np.float32(0.857), 'beta': np.float32(1.4519), 'delta': np.float32(0.9373), 'eta': np.float32(0.857), 'gamma': np.float32(0.0857), 'epsilon': np.float32(9.6997), 'd_effect': np.float32(0.4285)}\n",
      "Correct predictions:  4850/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|██████████| 40/40 [00:04<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 | Loss: 4.454308 \n",
      "params: {'alpha': np.float32(0.8548), 'beta': np.float32(1.3825), 'delta': np.float32(0.9363), 'eta': np.float32(0.8548), 'gamma': np.float32(0.0855), 'epsilon': np.float32(9.8327), 'd_effect': np.float32(0.4274)}\n",
      "Correct predictions:  4856/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100: 100%|██████████| 40/40 [00:03<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 | Loss: 4.396729 \n",
      "params: {'alpha': np.float32(0.8525), 'beta': np.float32(1.3318), 'delta': np.float32(0.9354), 'eta': np.float32(0.8525), 'gamma': np.float32(0.0853), 'epsilon': np.float32(9.9643), 'd_effect': np.float32(0.4263)}\n",
      "Correct predictions:  4849/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100: 100%|██████████| 40/40 [00:04<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 | Loss: 4.380966 \n",
      "params: {'alpha': np.float32(0.8503), 'beta': np.float32(1.2681), 'delta': np.float32(0.9345), 'eta': np.float32(0.8503), 'gamma': np.float32(0.085), 'epsilon': np.float32(10.0875), 'd_effect': np.float32(0.4251)}\n",
      "Correct predictions:  4853/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100: 100%|██████████| 40/40 [00:04<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 | Loss: 4.390823 \n",
      "params: {'alpha': np.float32(0.8481), 'beta': np.float32(1.2026), 'delta': np.float32(0.9333), 'eta': np.float32(0.8481), 'gamma': np.float32(0.0848), 'epsilon': np.float32(10.1954), 'd_effect': np.float32(0.424)}\n",
      "Correct predictions:  4855/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100: 100%|██████████| 40/40 [00:04<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 | Loss: 4.362090 \n",
      "params: {'alpha': np.float32(0.8459), 'beta': np.float32(1.1495), 'delta': np.float32(0.9325), 'eta': np.float32(0.8459), 'gamma': np.float32(0.0846), 'epsilon': np.float32(10.3013), 'd_effect': np.float32(0.423)}\n",
      "Correct predictions:  4841/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100: 100%|██████████| 40/40 [00:04<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 | Loss: 4.355523 \n",
      "params: {'alpha': np.float32(0.8438), 'beta': np.float32(1.0949), 'delta': np.float32(0.9315), 'eta': np.float32(0.8438), 'gamma': np.float32(0.0844), 'epsilon': np.float32(10.3993), 'd_effect': np.float32(0.4219)}\n",
      "Correct predictions:  4846/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100: 100%|██████████| 40/40 [00:04<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 | Loss: 4.296359 \n",
      "params: {'alpha': np.float32(0.8417), 'beta': np.float32(1.0448), 'delta': np.float32(0.9305), 'eta': np.float32(0.8417), 'gamma': np.float32(0.0842), 'epsilon': np.float32(10.4932), 'd_effect': np.float32(0.4208)}\n",
      "Correct predictions:  4877/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100: 100%|██████████| 40/40 [00:04<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 | Loss: 4.307471 \n",
      "params: {'alpha': np.float32(0.8396), 'beta': np.float32(0.9986), 'delta': np.float32(0.9308), 'eta': np.float32(0.8396), 'gamma': np.float32(0.084), 'epsilon': np.float32(10.5825), 'd_effect': np.float32(0.4198)}\n",
      "Correct predictions:  4848/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100: 100%|██████████| 40/40 [00:04<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 | Loss: 4.222018 \n",
      "params: {'alpha': np.float32(0.8375), 'beta': np.float32(0.9567), 'delta': np.float32(0.93), 'eta': np.float32(0.8375), 'gamma': np.float32(0.0838), 'epsilon': np.float32(10.6714), 'd_effect': np.float32(0.4188)}\n",
      "Correct predictions:  4869/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100: 100%|██████████| 40/40 [00:04<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 | Loss: 4.278197 \n",
      "params: {'alpha': np.float32(0.8355), 'beta': np.float32(0.9032), 'delta': np.float32(0.9299), 'eta': np.float32(0.8355), 'gamma': np.float32(0.0836), 'epsilon': np.float32(10.7475), 'd_effect': np.float32(0.4178)}\n",
      "Correct predictions:  4859/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100: 100%|██████████| 40/40 [00:04<00:00,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 | Loss: 4.263962 \n",
      "params: {'alpha': np.float32(0.8335), 'beta': np.float32(0.8596), 'delta': np.float32(0.93), 'eta': np.float32(0.8335), 'gamma': np.float32(0.0834), 'epsilon': np.float32(10.8246), 'd_effect': np.float32(0.4168)}\n",
      "Correct predictions:  4859/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100: 100%|██████████| 40/40 [00:04<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 | Loss: 4.265720 \n",
      "params: {'alpha': np.float32(0.8315), 'beta': np.float32(0.8094), 'delta': np.float32(0.9298), 'eta': np.float32(0.8315), 'gamma': np.float32(0.0832), 'epsilon': np.float32(10.8963), 'd_effect': np.float32(0.4158)}\n",
      "Correct predictions:  4845/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100: 100%|██████████| 40/40 [00:04<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 | Loss: 4.309920 \n",
      "params: {'alpha': np.float32(0.8296), 'beta': np.float32(0.768), 'delta': np.float32(0.9288), 'eta': np.float32(0.8296), 'gamma': np.float32(0.083), 'epsilon': np.float32(10.9629), 'd_effect': np.float32(0.4148)}\n",
      "Correct predictions:  4839/5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100: 100%|██████████| 40/40 [00:03<00:00, 11.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 | Loss: 4.253195 \n",
      "params: {'alpha': np.float32(0.8276), 'beta': np.float32(0.7237), 'delta': np.float32(0.9292), 'eta': np.float32(0.8276), 'gamma': np.float32(0.0828), 'epsilon': np.float32(11.0276), 'd_effect': np.float32(0.4138)}\n",
      "Correct predictions:  4856/5000 \n",
      "Early stopping triggered at epoch 57. Restoring best parameters from epoch 52.\n",
      "Final Weights: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': np.float32(0.84168327),\n",
       " 'beta': np.float32(1.0448098),\n",
       " 'delta': np.float32(0.93048894),\n",
       " 'eta': np.float32(0.84168327),\n",
       " 'gamma': np.float32(0.08416832),\n",
       " 'epsilon': np.float32(10.493217),\n",
       " 'd_effect': np.float32(0.42084163)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from Model import Retail_Recommendation\n",
    "\n",
    "## Reading datasets\n",
    "df = pd.read_parquet('../Data/data_with_features.parquet')\n",
    "customer = pd.read_pickle('../Data/customer_history.pkl')\n",
    "items = pd.read_pickle('../Data/item_summary.pkl')\n",
    "baskets = pd.read_pickle('../Data/baskets.pkl')\n",
    "itemsets = joblib.load('../Models/itemsets.joblib')\n",
    "rules = pd.read_pickle('../Data/rules.pkl')\n",
    "vectorizer = joblib.load('../Models/vectorizer.joblib')\n",
    "\n",
    "# coefficients to train\n",
    "params = {'alpha': 1.0, 'beta': 1.0, 'delta': 1.0, 'eta': 1.0, 'gamma': 0.1, 'epsilon': 2.0, 'd_effect': 0.5}\n",
    "\n",
    "# Calculating parameters\n",
    "Today = pd.Timestamp(df['Purchase Date'].max()) + pd.DateOffset(days=1)\n",
    "Time_period = (df['Purchase Date'].max() - df['Purchase Date'].min()).days\n",
    "d_effect = 0.5 # decay factor\n",
    "EPS = 1e-9\n",
    "\n",
    "Discount_dict = {} #{item: math.pow(np.random.default_rng().uniform(),(np.random.randint(1, 5))) for item in items.sample(100)['StockCode'].unique()}\n",
    "\n",
    "# Training model from class methods\n",
    "model = Retail_Recommendation(\n",
    "    items_data = items,\n",
    "    customer_data = customer,\n",
    "    rules = rules,\n",
    "    initial_weights = params,\n",
    "    d_effect = 0.5,\n",
    "    current_date = Today,\n",
    "    Time_period = 100,\n",
    "    filter_items = False,\n",
    "    include_description = True,\n",
    "    iteration_bar = False\n",
    ")\n",
    "\n",
    "Baskets = baskets[ baskets['Num products']>= 5].drop(columns=['Purchase Date', 'Purchase Time', 'InvoiceDate', 'Quantity', 'Amount', 'Total amount'])\n",
    "# we assume all purchases will be made today -> recency bias will not be trained well.\n",
    "Baskets['X'] = Baskets['StockCode'].apply(lambda x: x[:-1])\n",
    "Baskets['Y'] = Baskets['StockCode'].apply(lambda x: x[-1])\n",
    "\n",
    "\n",
    "Baskets['Total Cost'] = Baskets['Price'].apply(np.sum)\n",
    "# randomly scale up.down total cost to simulate budget\n",
    "# ±20% variation (uniform)\n",
    "variation = np.random.uniform(0.9, 1.25, size=len(Baskets))\n",
    "Baskets['pseudo_budget'] = Baskets['Total Cost'] * variation\n",
    "\n",
    "# Splitting into training and testing data\n",
    "train_baskets, test_baskets = train_test_split(Baskets, shuffle=True, test_size=0.2, random_state=42)\n",
    "\n",
    "optimal_weights, history = model.train_model(train_baskets, lr=0.05, n_iter=100,\n",
    "    error=1e-6, sample_size=5000, batch_size=128, top_n=100, budget_column='pseudo_budget',\n",
    "    Learning_rate_decay=0.02, reg_lambda=0.05, clip_value=5, early_stopping=True, patience=5)\n",
    "print(\"Final Weights: \")\n",
    "optimal_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8ebf184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'alpha': np.float32(0.84168327), 'beta': np.float32(1.0448098), 'delta': np.float32(0.93048894), 'eta': np.float32(0.84168327), 'gamma': np.float32(0.08416832), 'epsilon': np.float32(10.493217), 'd_effect': np.float32(0.42084163)}\n",
      "Best accuracy:  4877 / 5000\n",
      "Total number of items:  4888\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters: \", model.best_params)\n",
    "print(\"Best accuracy: \", model.best_result,'/', 5000)\n",
    "\n",
    "print(\"Total number of items: \", len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aca814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # Store best params\n",
    "# with open('../Models/best_parameters.pkl', 'wb') as f:\n",
    "#     pickle.dump(optimal_weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a5497c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': np.float32(0.84168327),\n",
       " 'beta': np.float32(1.044839),\n",
       " 'delta': np.float32(0.9319789),\n",
       " 'eta': np.float32(0.84168327),\n",
       " 'gamma': np.float32(0.08416832),\n",
       " 'epsilon': np.float32(10.493215),\n",
       " 'd_effect': np.float32(0.42084163)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pickle\n",
    "\n",
    "# par = pd.read_pickle('../Models/best_parameters.pkl')\n",
    "# par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2dacaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([h[0] for h in history])\n",
    "\n",
    "plt.title(\"Loss curve over epochs\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694475ed",
   "metadata": {},
   "source": [
    "Our model is overfitting which is causing our performance to not improve after a certain point.\n",
    "So, we will implement some regularization to prevent overfitting.\n",
    "\n",
    "1) L2 regularization factor\n",
    "2) Gradient clipping for stable training\n",
    "3) Learning rate decay factor\n",
    "4) Early stopping based on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d125394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(Model_rec, Carts, lr=0.05, n_iter=20, error=1e-6, sample_size=None,\n",
    "            batch_size=32, top_n=50, budget_column=None, verbose=True,\n",
    "            Learning_rate_decay=0, reg_lambda=0.0, clip_value=None, early_stopping=False, patience=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - Carts: DataFrame with baskets data\n",
    "            - lr: learning rate\n",
    "            - n_iter: number of gradient descent iterations\n",
    "            - error: Minimum error threshold for each item\n",
    "            - sample_size: size of sample for training our model\n",
    "            \n",
    "            - batch_size: size of batches for prediction at each iteration\n",
    "            - top_n: number of items to recommend\n",
    "            \n",
    "            - budget_column: Column name for budget value in our baskets dataset\n",
    "        \n",
    "        Parameters for regularization:\n",
    "            - Learning_rate_decay: learning rate decay\n",
    "            - reg_lambda: L2 regularization strength (0 => no reg)\n",
    "            - clip_value: gradient clipping limit (None or 0 => no clipping)\n",
    "            - early_stopping: enable early stopping\n",
    "            - patience: early stopping patience (epochs)\n",
    "        \"\"\"\n",
    "        # save learning rate decay param as before\n",
    "        Model_rec.lr_decay = Learning_rate_decay\n",
    "        \n",
    "        params = Model_rec.best_params.copy()\n",
    "        params['d_effect'] = Model_rec.d_effect\n",
    "        history = []\n",
    "        \n",
    "        # Early stopping setup\n",
    "        best_val_loss = np.inf\n",
    "        patience_counter = 0\n",
    "        \n",
    "        Model_rec.best_result = 0\n",
    "\n",
    "        def map_targets_to_indices(Y_batch):\n",
    "            return np.array([Model_rec.item_to_index.get(y, -1) for y in Y_batch], dtype=int)\n",
    "\n",
    "        for epoch in range(1, n_iter + 1):\n",
    "            # Optional sampling for speed\n",
    "            if sample_size is not None and sample_size < len(Carts):\n",
    "                Carts_epoch = Carts.sample(sample_size, random_state=epoch)\n",
    "            else:\n",
    "                Carts_epoch = Carts\n",
    "\n",
    "            n_batches = int(np.ceil(len(Carts_epoch) / batch_size))\n",
    "\n",
    "            total_loss = 0.0\n",
    "            total_count = 0\n",
    "            correct_total = 0\n",
    "            grads = {key: 0.0 for key in params.keys()}\n",
    "            \n",
    "            if Model_rec.include_description:\n",
    "                all_desc = Model_rec.compute_description_boost(Carts_epoch)\n",
    "\n",
    "            loop = range(n_batches) if not verbose else tqdm.tqdm(range(n_batches), desc=f\"Epoch {epoch}/{n_iter}\")\n",
    "            for b in loop:\n",
    "                batch = Carts_epoch.iloc[b * batch_size:(b + 1) * batch_size]\n",
    "                if batch.empty:\n",
    "                    continue\n",
    "                \n",
    "                # extract budget if applied\n",
    "                budget_for_batch = None\n",
    "                if budget_column and budget_column in batch.columns:\n",
    "                    budget_for_batch = batch[budget_column].values\n",
    "\n",
    "                X_batch = batch.drop(columns=['Y'])\n",
    "                feats = Model_rec._compute_features_batch(X_batch)\n",
    "                Y_batch = batch['Y'].values\n",
    "                Y_idx = map_targets_to_indices(Y_batch)\n",
    "                valid_mask = (Y_idx != -1)\n",
    "\n",
    "                # --- Forward ---\n",
    "                logit = (Model_rec.Bias\n",
    "                         + params['alpha'] * feats['H']\n",
    "                         + params['beta'] * feats['R']\n",
    "                         + params['gamma'] * feats['T']\n",
    "                         + params['delta'] * feats['P']\n",
    "                         + params['eta'] * feats['D']).astype(np.float32)\n",
    "\n",
    "                if Model_rec.include_description and Model_rec.weights['epsilon'] != 0.0:\n",
    "                    Desc_mat = np.vstack([all_desc[i] for i in X_batch.index]) # shape [batch_size, n_items]\n",
    "                    # transformation in your model: epsilon * log(1 + C)\n",
    "                    Desc_log = np.log1p(Desc_mat + error)\n",
    "                    logit += Model_rec.weights['epsilon'] * Desc_log\n",
    "\n",
    "                logit = logit.astype(np.float32)\n",
    "                logit -= logit.max(axis=1, keepdims=True)\n",
    "                exps = np.exp(logit)\n",
    "                probs = exps / (exps.sum(axis=1, keepdims=True) + EPS) # [batch_size, n_items]\n",
    "\n",
    "                # --- Loss ---\n",
    "                row_ids = np.arange(len(Y_batch))[valid_mask]\n",
    "                p_y = probs[row_ids, Y_idx[valid_mask]]\n",
    "\n",
    "                if top_n != -1 and top_n < Model_rec.n_items:\n",
    "                    part = np.argpartition(probs, -top_n, axis=1)[:, -top_n:]\n",
    "                    order = np.argsort(probs[np.arange(probs.shape[0])[:, None], part], axis=1)[:, ::-1]\n",
    "                    top_idx_matrix = np.take_along_axis(part, order, axis=1)\n",
    "                    isin_top = np.any(top_idx_matrix == Y_idx[:, None], axis=1)\n",
    "                    \n",
    "                    if isin_top.any():\n",
    "                        idxs_ok = row_ids[isin_top]\n",
    "                        total_loss += -np.log(probs[idxs_ok, Y_idx[valid_mask][isin_top]] + EPS).sum()\n",
    "                    \n",
    "                    penalized_count = (~isin_top).sum()\n",
    "                    if penalized_count > 0:\n",
    "                        total_loss += (-np.log(error)) * penalized_count\n",
    "                else:\n",
    "                    total_loss += -np.log(p_y + EPS).sum()\n",
    "\n",
    "                total_count += valid_mask.sum()\n",
    "\n",
    "                # --- Accuracy ---\n",
    "                if top_n == -1 or top_n >= Model_rec.n_items:\n",
    "                    top_idx_matrix_full = np.argsort(probs, axis=1)[:, ::-1]\n",
    "                else:\n",
    "                    part = np.argpartition(probs, -top_n, axis=1)[:, -top_n:]\n",
    "                    order = np.argsort(probs[np.arange(probs.shape[0])[:, None], part], axis=1)[:, ::-1]\n",
    "                    top_idx_matrix_full = np.take_along_axis(part, order, axis=1)\n",
    "                \n",
    "                topn_codes = Model_rec.all_items[top_idx_matrix_full]\n",
    "                true_codes = Y_batch[valid_mask][:, None]\n",
    "                valid_topn_codes = topn_codes[valid_mask]\n",
    "                correct_total += int(np.sum(np.any(valid_topn_codes == true_codes, axis=1)))\n",
    "\n",
    "                # --- Gradients ---\n",
    "                one_hot = np.zeros_like(probs ,dtype=np.float32)\n",
    "                one_hot[np.arange(len(Y_batch))[valid_mask], Y_idx[valid_mask]] = 1.0\n",
    "                \n",
    "                # derivative of cross-entropy wrt logits\n",
    "                diff = (probs - one_hot) / len(Y_batch)\n",
    "\n",
    "                grads['alpha'] += np.sum(diff * feats['H'])\n",
    "                grads['beta'] += np.sum(diff * feats['R'])\n",
    "                grads['gamma'] += np.sum(diff * feats['T'])\n",
    "                grads['delta'] += np.sum(diff * feats['P'])\n",
    "                grads['eta'] += np.sum(diff * feats['D'])\n",
    "                \n",
    "                if Model_rec.include_description and Model_rec.weights['epsilon'] != 0.0:\n",
    "                    grads['epsilon'] = grads.get('epsilon', 0.0) + np.sum(diff * Desc_log)\n",
    "                else:\n",
    "                    grads['epsilon'] = grads.get('epsilon', 0.0) # ensure key exists\n",
    "\n",
    "                # feats['T'] = 1 + d_effect * exp(-K * delta) \n",
    "                # dT/d(d_effect) = exp(-K * delta)\n",
    "                # so d(logit)/ dT * dT/d(d_effect) =  d(logit)/d(d_effect) = gamma * exp(-K * delta)\n",
    "                # compute exp(-K * delta) safely: feats['T'] - 1 = d_effect * exp(-K * delta)  => exp(-K * delta) = (T - 1)/d_effect  (d_effect used to compute feats)\n",
    "                \n",
    "                d_eff_val = float(params.get('d_effect', 0.0))\n",
    "                if abs(d_eff_val) > 1e-12:\n",
    "                    E_mat = (feats['T'] - 1.0) / (d_eff_val + EPS)\n",
    "                else:\n",
    "                    # fallback: if d_effect is ~0 , approximate exp(-K * delta) as T-1 because T ≈ 1 + small*E\n",
    "                    E_mat = (feats['T'] - 1.0)\n",
    "                grads['d_effect']  = grads.get('d_effect', 0.0) + np.sum(diff * (params['gamma']* E_mat))\n",
    "\n",
    "            \n",
    "            # --- L2 Regularization ---\n",
    "            if reg_lambda > 0:\n",
    "                l2_term = sum(v**2 for v in params.values())\n",
    "                total_loss += reg_lambda * l2_term # adding l2 term to loss\n",
    "\n",
    "                # Update gradient to include L2 term\n",
    "                for k in params.keys():\n",
    "                    grads[k] += 2.0 * reg_lambda * params[k]\n",
    "                    \n",
    "            # --- Gradient Clipping ---\n",
    "            if clip_value is not None:\n",
    "                for k in grads.keys():\n",
    "                    grads[k] = np.clip(grads[k], -clip_value, clip_value)\n",
    "            \n",
    "            # Normalize + update\n",
    "            if total_count == 0:\n",
    "                if verbose:\n",
    "                    print(\"No valid samples; stopping early.\")\n",
    "                break\n",
    "            \n",
    "            effective_lr = lr / (1 + Model_rec.lr_decay * epoch) # learning rate decay\n",
    "            for k in grads.keys():\n",
    "                params[k] -= effective_lr * grads[k]\n",
    "\n",
    "            Model_rec.weights = {k: v for k, v in params.items() if k != 'd_effect'}\n",
    "            Model_rec.d_effect = params['d_effect']\n",
    "\n",
    "            epoch_loss = total_loss / max(1, total_count)\n",
    "            epoch_acc = correct_total / max(1, total_count)\n",
    "            history.append((epoch_loss, epoch_acc, copy.deepcopy(params)))\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch}/{n_iter} | Loss: {epoch_loss:.6f} \") # | Acc: {epoch_acc:.4f}\n",
    "                print(f\"params: { {k: round(v,4) for k,v in params.items()} }\")\n",
    "                print(f\"Correct predictions:  {correct_total}/{total_count} \"  )\n",
    "                \n",
    "            # --- Early stopping evaluation (validate and possibly stop) ---\n",
    "            if early_stopping:\n",
    "                if epoch_loss + error < best_val_loss: # reset counter\n",
    "                    patience_counter = 0\n",
    "                    best_val_loss = epoch_loss\n",
    "                    best_epoch = epoch\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping triggered at epoch {epoch}. Restoring best parameters from epoch {best_epoch}.\")\n",
    "                    Model_rec.weights = {k: v for k, v in Model_rec.best_params.items() if k != 'd_effect'}\n",
    "                    Model_rec.d_effect = Model_rec.best_params['d_effect']\n",
    "                    params = copy.deepcopy(Model_rec.best_params)\n",
    "                    break\n",
    "            \n",
    "            if correct_total > Model_rec.best_result:\n",
    "                Model_rec.best_result = correct_total\n",
    "                Model_rec.best_params = copy.deepcopy(params)\n",
    "            \n",
    "\n",
    "        final_weights = copy.deepcopy(params)\n",
    "        return final_weights, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f252e",
   "metadata": {},
   "source": [
    "Recency is a skewed parameter which is actually dominating everything else and corrupting our model <br>\n",
    "So, we will put initial weight for recency to be very low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b02560a",
   "metadata": {},
   "source": [
    "### Part-6: Model evaluation\n",
    "Now, we will make functions to evaluate our recommendation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a6d34",
   "metadata": {},
   "source": [
    "After thinking and reading about recommendation model, we have the following metrics for evaluating recommendation model:\n",
    "1. **Precision@N** → Measures how many of the top-N recommended items are actually relevant.  \n",
    "  $ \\text{Precision}@N \\;=\\; \\frac{\\#\\text{ relevant items in top-}N}{N} $\n",
    "  - Higher precision means your recommendations are more accurate and focused.\n",
    "\n",
    "2. **Recall@N** → Measures how many of the relevant (true) items were captured within the top-N recommendations.  \n",
    "  $ \\text{Recall}@N \\;=\\; \\frac{\\#\\text{ relevant items in top-}N}{\\#\\text{ of all relevant items}} $\n",
    "  - *(For next-item prediction where there's one true item per basket, Recall@N ≈ HitRate@N.)*\n",
    "\n",
    "3. **F1@N Score** → Harmonic mean of Precision@N and Recall@N.  \n",
    "  $ \\text{F1}@N \\;=\\; 2 \\times \\frac{\\text{Precision}@N \\times \\text{Recall}@N}{\\text{Precision}@N + \\text{Recall}@N} $\n",
    "  - A balanced measure of both accuracy and completeness of recommendations.\n",
    "\n",
    "4. **HitRate@N** → Fraction of test baskets where the true next item appeared in the top-N list.  \n",
    "  $ \\text{HitRate}@N \\;=\\; \\frac{\\#\\text{ baskets with a correct hit}}{\\text{total baskets}} $\n",
    "  - Indicates how often the system \"gets it right\" for users.\n",
    "\n",
    "5. **Mean Reciprocal Rank (MRR@N)** → Measures the average of reciprocal ranks of the true items in the recommended lists.  \n",
    "   $ \\text{MRR}@N \\;=\\; \\frac{1}{|B|} \\sum_{i=1}^{|B|} \\frac{1}{\\text{rank of true item in top-N}} $  \n",
    "   - Gives more weight to correct items appearing higher in the recommendation list.\n",
    "\n",
    "6. **Normalized Discounted Cumulative Gain (NDCG@N)** → Measures ranking quality, giving higher score when relevant items appear at the top.  \n",
    "   $ \\text{NDCG}@N \\;=\\; \\frac{DCG}{IDCG}, \\quad DCG = \\sum_{i=1}^{N} \\frac{2^{rel_i}-1}{\\log_2(i+1)} $  \n",
    "   - Higher NDCG → better ranking of relevant items.\n",
    "\n",
    "7. **Mean Average Precision (MAP@N)** → Average precision at ranks where relevant items appear, averaged across all test baskets.  \n",
    "   $ \\text{MAP}@N \\;=\\; \\frac{1}{|B|} \\sum_{b \\in B} AP_b $  \n",
    "   - Rewards both correctness and higher ranking of relevant items.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In summary:**\n",
    "- High **Precision** → fewer wrong recommendations.  \n",
    "- High **Recall / HitRate** → the model rarely misses what the user actually wanted.  \n",
    "- High **F1** → good balance between both.\n",
    "- High **MRR@N, NDCG@N, MAP@N** → ensures correct items are ranked higher, improving user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1dd4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate_model(rec_model, test_baskets, top_n=10, batch_size=128, budget_column='pseudo_budget', error=1e-8):\n",
    "    \"\"\" Evaluate our recommednation rec_model on unseen baskets\n",
    "    Computes metrics: \n",
    "        - Precision, Recall, F1,\n",
    "        - Hits@N (fraction where true item appears in top-N recommendations)\n",
    "    Parameters\n",
    "    ----------\n",
    "    rec_model : Retail_Recommendation\n",
    "    test_baskets : pd.DataFrame\n",
    "        Test baskets dataframe\n",
    "    top_n : int\n",
    "        Number of top items to consider from recommendations\n",
    "    batch_size : int\n",
    "        Number of baskets to process in one batch\n",
    "    budget_column : str\n",
    "        Column name for budget\n",
    "    error: float\n",
    "        minimum error associate with each quantity for failsafe\n",
    "    Returns a dict of aggregated scores.\n",
    "    \"\"\"\n",
    "    if test_baskets.empty: \n",
    "        raise ValueError(\"Test baskets dataframe is empty.\")\n",
    "           \n",
    "    # Handle small batch issue safely\n",
    "    if batch_size is not None:\n",
    "        batch_size = min(batch_size, len(test_baskets))\n",
    "    \n",
    "    # Run rec_model inference\n",
    "    results = rec_model.recommend(test_baskets, top_n=top_n, batch_size=batch_size, budget_column=budget_column)\n",
    "    \n",
    "    hits = 0\n",
    "    total_true = len(test_baskets)\n",
    "    precisions, recalls, f1s = [], [], []\n",
    "    reciprocal_ranks, ndcgs, average_precisions = [], [], []\n",
    "    for idx, row in test_baskets.iterrows():\n",
    "        true_item = row['Y']\n",
    "        if idx not in results: # No prediction for this basket\n",
    "            continue \n",
    "                   \n",
    "        Result = results[idx] # predicted items\n",
    "        if 'Rank' in Result.columns:\n",
    "            ranks_item = Result.sort_values(by='Rank')['StockCode'].tolist()[:top_n]\n",
    "        else:\n",
    "            ranks_item = Result['StockCode'].tolist()[:top_n]            \n",
    "        if not ranks_item: continue\n",
    "        \n",
    "        # --- Hit@N ---\n",
    "        hits_i = 1 if true_item in ranks_item else 0\n",
    "        hits += hits_i            \n",
    "        precision_i = hits_i / len(ranks_item)\n",
    "        recall_i = hits_i / 1.0\n",
    "        f1_i = (2 * precision_i * recall_i) / (precision_i + recall_i + error)\n",
    "        \n",
    "        precisions.append(precision_i)\n",
    "        recalls.append(recall_i)\n",
    "        f1s.append(f1_i)\n",
    "        \n",
    "        # --- Rank-sensitive metrics ---\n",
    "        if hits_i:\n",
    "            rank = ranks_item.index(true_item) + 1\n",
    "            \n",
    "            # MRR: Reciprocal rank of true item\n",
    "            reciprocal_ranks.append(1.0 / rank)\n",
    "            \n",
    "            # NDCG: discounted gain normalized by ideal DCG (1.0)\n",
    "            dcg = 1.0 / math.log2(rank + 1)\n",
    "            ndcgs.append(dcg)\n",
    "            \n",
    "            # Average Precision: since 1 true item, AP = precision at that rank\n",
    "            ap = 1.0 / rank\n",
    "            average_precisions.append(ap)\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "            ndcgs.append(0.0)\n",
    "            average_precisions.append(0.0)\n",
    "        \n",
    "    metrics = {\n",
    "        'Precision@N': np.mean(precisions) if precisions else 0.0,\n",
    "        'Recall@N': np.mean(recalls) if recalls else 0.0,\n",
    "        'F1@N': np.mean(f1s) if f1s else 0.0,\n",
    "        'HitRate': hits / max(1, total_true),\n",
    "        'MRR@N': np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0,\n",
    "        'NDCG@N': np.mean(ndcgs) if ndcgs else 0.0,\n",
    "        'MAP@N': np.mean(average_precisions) if average_precisions else 0.0,\n",
    "        'Fraction correct as ratio': f\"{hits}/{total_true}\",\n",
    "    }\n",
    "    rec_model.eval_metrics = metrics\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77d8c4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Precision@N': np.float64(0.008437499999999999),\n",
       " 'Recall@N': np.float64(0.84375),\n",
       " 'F1@N': np.float64(0.01670792062665425),\n",
       " 'HitRate': 0.027190332326283987,\n",
       " 'MRR@N': np.float64(0.10176132668649739),\n",
       " 'NDCG@N': np.float64(0.23397873859672863),\n",
       " 'MAP@N': np.float64(0.10176132668649739),\n",
       " 'Fraction correct as ratio': '108/3972'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluation_metrics = Evaluate_model(model, test_baskets.reset_index(drop=True), top_n=100, batch_size=128, budget_column='pseudo_budget', error=1e-8)\n",
    "print(\"Evaluation Metrics: \")\n",
    "Evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b0173",
   "metadata": {},
   "source": [
    "For adjusting this into our `model.py` class,\n",
    "1. We will create a new function `Evaluate_model` in our `Model.py` class.\n",
    "2. We will remodel recommend function to work with single basket and also with a batch of baskets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ca56959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@N': np.float64(0.00859375),\n",
       " 'Recall@N': np.float64(0.859375),\n",
       " 'F1@N': np.float64(0.017017326564184886),\n",
       " 'HitRate': 0.02769385699899295,\n",
       " 'MRR@N': np.float64(0.108077055760611),\n",
       " 'NDCG@N': np.float64(0.2423474102909143),\n",
       " 'MAP@N': np.float64(0.108077055760611),\n",
       " 'Fraction correct as ratio': '110/3972'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.Evaluate_model(test_baskets, top_n=100, batch_size=128, budget_column='pseudo_budget', error=1e-8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
