{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830e6e56",
   "metadata": {},
   "source": [
    "## Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee694ba1",
   "metadata": {},
   "source": [
    "### Part-4: Final Recommendation\n",
    "\n",
    "For my recommendation algo, I am taking inspiration from (boosting + naive bayes) where, <br>\n",
    "every variable is asigned equal weight -> weight's are changed bases on items importance [previous purchase frequency] -> weight are resampled with new imp. [association with current items] -> <br>\n",
    "normalization to get probability for each item -> sorting for top items -> final recommendation.\n",
    "\n",
    "> For simplicity, we won't recommend rare item. For complete inventory recommendation, replace df with original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16bd1a",
   "metadata": {},
   "source": [
    "**Note**: <br>\n",
    "\n",
    "By default, you might think all items are eqaully probable. But this is not the case. <br>\n",
    "Frequency of items purchase is very different. To get actual popularity of item aka frequency of occurance, <br>\n",
    "We need to find quantity of items bought per transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f90ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7461af50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique items: 263\n"
     ]
    }
   ],
   "source": [
    "## Reading datasets\n",
    "df = pd.read_parquet('../Data/data_with_features.parquet')\n",
    "customer = pd.read_pickle('../Data/customer_history.pkl')\n",
    "items = pd.read_pickle('../Data/item_summary.pkl')\n",
    "baskets = pd.read_pickle('../Data/baskets.pkl')\n",
    "itemsets = joblib.load('../Models/itemsets.joblib')\n",
    "rules = pd.read_pickle('../Data/rules.pkl')\n",
    "vectorizer = joblib.load('../Models/vectorizer.joblib')\n",
    "\n",
    "unq_item = len(df['StockCode'].unique())\n",
    "print(\"Total unique items:\", unq_item)\n",
    "\n",
    "# ## Remove rare item (Optional)\n",
    "# customer = customer[customer['Customer ID'].isin(df['Customer ID'].unique())]\n",
    "# items = items[items['StockCode'].isin(df['StockCode'].unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d395ee",
   "metadata": {},
   "source": [
    "#### My Current Model scheme:\n",
    "I will create a model which modifies the probability of each item being bought based on multiple factors. <br>\n",
    "This model will initial a probability vector for each customer based on their purchase history and association rules.\n",
    "\n",
    "This probability vector will then be used to predict the probability of each item being bought by the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba25b6",
   "metadata": {},
   "source": [
    "Let base probability of an item be $P_0‚Äã(i)$.\n",
    "\n",
    "We want an updated $ùëÉ(ùëñ)$ that reflects:\n",
    "\n",
    "- Customer affinity: $H_i$\n",
    "- Rule-based affinity: $R_i$\n",
    "\n",
    "\n",
    "Formally:\n",
    "$$ P(i) ‚àù P_0(i) √ó f(H_i) √ó g(R_i) $$\n",
    "\n",
    "Now, we must choose f and g ‚Äî the transformation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f41744",
   "metadata": {},
   "source": [
    "üî∏ Option A: Empending histories effect with direct rule boost\n",
    "\n",
    "f: log1p(alpha * (qty/count)) <br>\n",
    "g: (1 + beta * lift)\n",
    "So,\n",
    "$$ P(i) = P_0(i) √ó log(1+\\alpha. \\frac{qty_i}{count_i}) √ó (1 + \\beta.lift_i) $$\n",
    "\n",
    "üî∏ Option B: Use log-space additive updates\n",
    "Taking everything in log-space, so multiplicative effects become additive.\n",
    "This is stand in probabilistic models (e.g., Naive Bayes, word2vec)\n",
    "$$ logP(i) = logP_0(i) √ó \\alpha.log(1+\\frac{qty_i}{count_i}) √ó \\beta.log(lift_i) $$\n",
    "\n",
    "üî∏ Option C: Weighted geometric mean\n",
    "Blended normal multiplication\n",
    "$$ P(i) = P_0(i)^{1-\\alpha -\\beta} √ó f(H_i)^\\alpha √ó g(R_i)^\\beta $$\n",
    "- This keeps balance b/w base, history, and rule weights\n",
    "- Good for ensemble-like blending (like XGBoost feature combination)\n",
    "\n",
    "üî∏ Option D: Bayesian update analogy\n",
    "Interpret rules and history as independent evidence sources for item likelihood.\n",
    "$$ P(i|history,rules) ‚àù P_0(i) √ó P(history|i)^\\alpha √ó P(rules|i)^\\beta $$\n",
    "Then define:\n",
    "- $P(history|i)$ = normalized purchase frequency\n",
    "- $P(rules|i)$ = lift or confidence\n",
    "\n",
    "IF we normalize both, this can be approximated as:\n",
    "$$ P(i) = P_0(i) √ó (1 +\\alpha.H_i) √ó (1 +\\beta.R_i) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1801adb",
   "metadata": {},
   "source": [
    "Upon careful observation, we see that for building a scalable recommender system which has\n",
    "1) Interpretability\n",
    "2) consistent probabilistic meanings and\n",
    "3) stability across scales\n",
    "\n",
    "then the log-space additive model B is the best choice.\n",
    "\n",
    "#### **Recommended Formula**\n",
    "$$ logP(i) = logP_0(i) √ó \\alpha.log(1+\\frac{qty_i}{count_i}) √ó \\beta.log(lift_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f4ebb6",
   "metadata": {},
   "source": [
    "Also, I also research into two fundamental mathematical question for finding best formula.\n",
    "\n",
    "1) Should alpha/beta be inside the log or outside the log? <br>\n",
    "ANSWER: Outside the log\n",
    "\n",
    "- when tuning parameters are outside - you take the log-evidence `log(1+x)` and scale its importance linearly.\n",
    "  In log-space it reads as adding alpha times the evidence:\n",
    "  $$ logP ‚Üí logP + \\alpha.log(1+x) $$\n",
    "‚Üí Alpha controls how many 'units of evidence' that history constributes. It's linear in log domain and easy to tune\n",
    "\n",
    "- when tuning parameters are inside - you take the log-evidence `log(1+x)` and scale its importance non-linearly. <br>\n",
    "  For small x, `log(1+\\alpha*x)` ‚âà `log(1+x)`, but for large x, `log(1+\\alpha*x)` is close to `log(1+\\alpha)`.\n",
    "  i.e., it saturated differently as it couples the scale which makes it unstable.\n",
    "\n",
    "1) Lift v/s confidence*lift - which to use? <br>\n",
    "Answer: No blind answer. Treat `lift` and `confidence` as complementart signals and combine them in log-space with seperate weights.\n",
    "\n",
    "- Lift and confidence may have common part in formula but they measure different things and have different dimension.\n",
    "- Multipling them mixed different scales -> product will be dominated by large lifts for very rare items or by moderately high confidence. <br>\n",
    "  It is a blunt instrument and can overweight noisy rules.\n",
    "- Better options: either use confidence as a weight in lift or both as additive terms with separate tunable weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcad86",
   "metadata": {},
   "source": [
    "After multiple iteration, i have reached an even better solution. I will use this method for the rest of the notebook.\n",
    "\n",
    "#### **Formula**:\n",
    "Probability vector for each item(i), for each user(u) -> $P_{u,i}$ is the probability of the item being bought by the user.\n",
    "$$ P_{u,i} = Base_i + \\sum{weight_{f,i} * function(feature_{u,i})} $$\n",
    "\n",
    "-> P_{u,i} = log-space additive features * linear-space multiplicative features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5007161",
   "metadata": {},
   "source": [
    "#### Probability vector for each item depend upon:\n",
    "##### A. log-space additive features:\n",
    "1. Base probability of an item = $P_0(i)$ -> Item bias($b(i)$) <br>\n",
    "   $$ b_i = log(1 + \\frac{total quantity_i}{num orders_i}) $$\n",
    "\n",
    "2. Frequency of previous purchase of customer history = $H_i$ -> Customer affinity <br>\n",
    "   $$ H_i = log(1 + \\frac{qty_{u, i}}{count_{u, i}}) $$\n",
    "\n",
    "3. Boost due to association rules = $R_i$ -> Rule-based affinity <br>\n",
    "   $$ R_i = confidence_i * log(1 + lift_i) $$\n",
    "\n",
    "4. Price Affinity = $P_i$ -> effect of price <br>\n",
    "   $$ P_i = log(1 + \\frac{price_i - budget_u}{price_i}) $$\n",
    "\n",
    "5. Discount effect = $D_i$ -> effect of discount (when given) <br> \n",
    "   $$ D_i = log(1 + discount_i) $$\n",
    "\n",
    "6. Recency time-based factor = $T_i$ -> effect of last purchase transaction <br>\n",
    "   $$ T_i = log(1+ d_{effect}*exp(- k_i \\Delta{t{(u,i)}}) ) $$\n",
    "   where $d_{effect}$ - decay effect, $k_i$ - recency factor.\n",
    "\n",
    "   $$ t_i = 1+ d_{effect}*exp(- k_i \\Delta{t{(u,i)}}) $$\n",
    "##### B. linear-space multiplicative features:\n",
    "1. Description commonality = $C_i$ -> effect of description <br>\n",
    "   $$ C_i = cosine(desc_i, desc_{other}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0266343f",
   "metadata": {},
   "source": [
    "> ##### Why add 1 before multiplying Description similarity? <br>\n",
    "Even after normalization, we want the minimum effect to be unity as Multiplying by raw $D_i$ (0‚Äì1) would shrink the score for items that are not similar, possibly too aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb3f2b8",
   "metadata": {},
   "source": [
    "Hence, the final probability of an item being bought by a customer is:\n",
    "$$ Z_{u,i} = exp(b_i + \\alpha*H_i + \\beta*R_i + \\delta*P_i + \\eta*D_i + \\gamma*T_i).(1 + C_i)^{\\epsilon} $$\n",
    "#### OR\n",
    "$$ Z_{u,i} = exp(b_i + \\alpha*H_i + \\beta*R_i + \\delta*P_i + \\eta*D_i). t_i^{\\gamma}.(1 + C_i)^{\\epsilon} $$\n",
    "Taking log both sides,\n",
    "$$ log(Z_{u,i}) = b_i + \\alpha*H_i + \\beta*R_i + \\delta*P_i + \\eta*D_i + \\gamma*T_i + \\epsilon*log(1 + C_i) $$\n",
    "$$ P_{u,i} = Normalization( log(Z_{u,i}) )  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8369726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell-1: Setup Hyper-parameters\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "## Initial weights\n",
    "ALPHA = 0.5 # purchase history\n",
    "BETA = 0.4 # association rules\n",
    "DELTA = 0.1 # recency bias\n",
    "ETA = 0.3 # discount boost\n",
    "GAMMA = 0.2 # price\n",
    "EPISILON = 0.3 # description similarity\n",
    "\n",
    "# coefficients to train\n",
    "params = {'alpha': 1.0, 'beta': 1.0, 'delta': 1.0, 'eta': 1.0, 'gamma': 1.0, 'epsilon': 1.0, 'd_effect': 0.5}\n",
    "\n",
    "## All additional hyper-parameters\n",
    "d_effect = 0.5 # decay factor\n",
    "\n",
    "def normalize_dict(d):\n",
    "    max_val = max(d.values()) if d else 1\n",
    "    return {k: v / max_val for k, v in d.items()}\n",
    "\n",
    "\n",
    "def Softmax_normalizer(x:dict):\n",
    "    val = np.array(list(x.values()))\n",
    "    val = (val - val.min())/(val.max() - val.min())\n",
    "    e_x = np.exp(val)\n",
    "    prob = e_x / e_x.sum()\n",
    "    return dict(zip(x.keys(), prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba0334",
   "metadata": {},
   "source": [
    "In softmax function, we subtract our datapoint but max value. This is numerical trick done to stablize function <br>\n",
    "Softmax works with exponential function which grows too large. This leads to one item dominating -> Overflow to `inf`\n",
    "\n",
    "So, we shift all logits by same constants to keep the largest value around 0 -> exp(0) -> result in unity as largest number of normalization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ef0ed",
   "metadata": {},
   "source": [
    "> Each additive features should be normalized for scale invariance across factors. <br>\n",
    "> Multiplicative features can remain as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12119438",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell-2: Base proabability/ item bias bi\n",
    "def compute_bias(items_df):\n",
    "    \"\"\"\n",
    "    Function to compute base probability for each item -> bias of an item\n",
    "\n",
    "    Args: items_df (pd.DataFrame): DataFrame containing item information.\n",
    "    \"\"\"\n",
    "    bias = {}\n",
    "    for _,item in items_df.iterrows():\n",
    "        bias[item['StockCode']] = np.log1p( item['Frequency'] )\n",
    "    \n",
    "    ## Normalize bias\n",
    "    bias = normalize_dict(bias)\n",
    "    return bias\n",
    "\n",
    "# compute_bias(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebda55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell-3: Previous customer history\n",
    "def compute_history(customer_df = customer,customer_id:int=-1):\n",
    "    \"\"\" Function to compute previous customer history. \"\"\"\n",
    "    history = {}\n",
    "\n",
    "    customer_data = customer_df[customer_df['Customer ID'] == customer_id]\n",
    "    if customer_data.empty:\n",
    "        return history\n",
    "    \n",
    "    ## Customer dataset has another layer for dictionary -> so we have to use .values[0]\n",
    "    count_dict = customer_data['Purchase count'].values[0]\n",
    "    qty_dict = customer_data['Purchase quantity'].values[0]\n",
    "    \n",
    "    for item in count_dict.keys():\n",
    "        count_item = count_dict[item]\n",
    "        qty_item = qty_dict[item]\n",
    "        # print(item, count_item, qty_item)\n",
    "        history[item] = np.log1p(qty_item / count_item)\n",
    "    \n",
    "    ## Normalize history\n",
    "    history = normalize_dict(history)\n",
    "    return history\n",
    "\n",
    "# compute_history(customer, customer_id=12608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b591d062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "## Cell-4: Association rules\n",
    "def compute_rules(current_basket, rules=rules, items=items):\n",
    "    \"\"\" \n",
    "    Function to compute rules association boost for each item. \n",
    "    \n",
    "    Args:\n",
    "        current_basket : List of items in the current basket.\n",
    "        rules: Dataset containing association rules.\n",
    "        items: Dataset containing item information.\n",
    "    \"\"\"\n",
    "    rule_boost = {} # map item -> boost\n",
    "    \n",
    "    if isinstance(current_basket, (list,set)): # only list of items\n",
    "        current_basket = set(current_basket)\n",
    "    elif hasattr(current_basket, 'StockCode') or 'StockCode' in current_basket.columns: # dataframe with other info\n",
    "        # hasattr for checking series\n",
    "        current_basket = set(current_basket['StockCode'])\n",
    "    else:\n",
    "        raise TypeError(\"current basket must be list,set or Dataframe with 'StockCode' column.\")\n",
    "\n",
    "    itemsets = set(list(combinations(current_basket, 1)) + list(combinations(current_basket,2)))\n",
    "    print(\"All Itemsets: \", itemsets)\n",
    "    \n",
    "\n",
    "    rules_index = {}\n",
    "    for _, row in rules.iterrows():\n",
    "        ant = tuple(sorted(row['antecedent']))  # ensure consistent tuple type\n",
    "        rules_index.setdefault(ant, []).append(row)\n",
    "    \n",
    "    \n",
    "    for itemset in itemsets:\n",
    "        ## for each itemset, find all its association rules\n",
    "        if itemset not in rules_index:\n",
    "            continue\n",
    "        else:\n",
    "            for row in rules_index[itemset]: # for each matching rule\n",
    "                # for each rules, get all resultant items\n",
    "                conseq = row['consequent']\n",
    "                for item in conseq:\n",
    "                    # for each resultant item, compute boost\n",
    "                    boost = row['confidence']* np.log1p(row['lift'])\n",
    "\n",
    "        ## If item is already in rule boost through other itemset, add to it\n",
    "                    if item in rule_boost:\n",
    "                        rule_boost[item] += boost\n",
    "                    else:\n",
    "                        rule_boost[item] = boost\n",
    "    \n",
    "    # normalization\n",
    "    rule_boost = normalize_dict(rule_boost)\n",
    "    return rule_boost\n",
    "\n",
    "compute_rules(baskets.iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321cc35a",
   "metadata": {},
   "source": [
    "Budget constraints should also penalize items that are over budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626275b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cell-5: Budget constraints\n",
    "def compute_price_bias(items=items, budget=None):\n",
    "    \"\"\"\n",
    "    Computes bias due to budget constraints\n",
    "    \n",
    "    Args:\n",
    "        items: Dataset containing item information.\n",
    "        budget: Budget for customer\n",
    "    \"\"\"\n",
    "    \n",
    "    if budget is None:\n",
    "        return {}\n",
    "    \n",
    "    price_bias = {}\n",
    "    for _,item in items.iterrows():\n",
    "        curr_price = item.get('Current_Price', 0)\n",
    "        # ratio = (budget - curr_price)/curr_price\n",
    "        ratio = (budget)/curr_price\n",
    "        price_bias[item['StockCode']] = np.log1p(ratio)\n",
    "    \n",
    "    ## Normalize bias\n",
    "    price_bias = normalize_dict(price_bias)\n",
    "    return price_bias\n",
    "\n",
    "# sns.displot(compute_price_bias(items, budget=50).values(), kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14aade4",
   "metadata": {},
   "source": [
    "> In case discount is given on items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cell-6: Discount's effect\n",
    "def compute_discount_boost(discount, items=items):\n",
    "    \"\"\" Function to compute discount boost for each item.\n",
    "    \n",
    "    Args:\n",
    "        discount (dictionary): Discount percentage on each item\n",
    "        items: Dataset containing item information.\n",
    "    \"\"\"\n",
    "    \n",
    "    discount_bias = {}\n",
    "    for _,item in items.iterrows():\n",
    "        item_name = item['StockCode']\n",
    "        discount_bias[item_name] = np.log1p(discount.get(item_name, 0))\n",
    "    \n",
    "    ## Normalize bias\n",
    "    max_val = max(discount_bias.values()) if discount_bias else 1\n",
    "    discount_bias = {k: v / max_val for k, v in discount_bias.items()}\n",
    "    \n",
    "    return dict(sorted(discount_bias.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ef723",
   "metadata": {},
   "source": [
    "#### How per-item recency variation works\n",
    "Recency factor works from the idea of decay effect. Our items proability of buying decays based on last purchase date of the customer.\n",
    "\n",
    "$$ T_i = d_{effect}*(1+ exp(- k_i \\Delta{t{(u,i)}}) ) $$\n",
    "\n",
    "- our items prob goes from 1 to d_effect over time\n",
    "- $d_{effect}$: decay factor which determines the overall magnitude of effect from decay\n",
    "- $k_i$: recency factor which determines the steepness of decay for each item\n",
    "- $k_i$ marks each item prob based on difference on purchase date from today\n",
    "\n",
    "##### Formula for k:\n",
    "$$ K_i = k_{min} + (k_{max} - k_{min}) * norm[ \\sqrt{log(1 + quantity bought_i)} ] $$\n",
    "where  k_min and k_max are global.\n",
    "\n",
    "- norm_logfreq_i ‚àà [0,1], computed from log1p(Num_orders)\n",
    "- Frequent items ‚Üí norm_logfreq ‚âà 1 ‚Üí k_i ‚âà k_max\n",
    "- Rare items ‚Üí norm_logfreq ‚âà 0 ‚Üí k_i ‚âà k_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38e050",
   "metadata": {},
   "source": [
    "NOTE: This dataset contains transcations from 2009 to 2011 -> long time ago. <br>\n",
    "So, we can't find difference from actual today. Thus we will train model for today being = last date in dataset + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75695dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Today = pd.Timestamp(df['Purchase Date'].max()) + pd.DateOffset(days=1)\n",
    "Time_period = (df['Purchase Date'].max() - df['Purchase Date'].min()).days\n",
    "\n",
    "## Cell-7: Recency bias -> users per items\n",
    "def compute_RecencyBias(items=items, customer=customer, customer_id=-1, d_effect=d_effect, today=Today, Time_period=Time_period):\n",
    "    \"\"\" \n",
    "    Function to compute recency bias for each item. = d_effect * (1 + exp(-k_i * delta))\n",
    "    \n",
    "    Args:\n",
    "        current_basket : List of items in the current basket.\n",
    "        items: Dataset containing item information.\n",
    "        customer: Dataset containing customer's data\n",
    "        customer_id: Customer ID (in case entire data is given)\n",
    "        d_effect: strength of recency decay\n",
    "        today: current date for time delta\n",
    "        Time_period: for maximum recency factor calculation\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(customer) == pd.core.frame.DataFrame:\n",
    "        customer_data = customer[customer['Customer ID'] == customer_id]\n",
    "    elif customer_id == -1: # if only current customer is given\n",
    "        # for anonymous customer, there is no recency bias\n",
    "        return {}\n",
    "    else: # not a dataframe -> direct customer data\n",
    "        customer_data = customer\n",
    "        \n",
    "    \n",
    "    kmin = np.log(2)/Time_period\n",
    "    kmax = np.log(2)/7\n",
    "    \n",
    "    # for all item, at least Ti = d_effect\n",
    "    recency_bias = defaultdict()\n",
    "    \n",
    "    freq = {item : 0 for item in items['StockCode']}\n",
    "    quantity = customer_data['Purchase quantity'].values[0]\n",
    "    \n",
    "    for item, qty in quantity.items():\n",
    "        # log(1+X) for 0 is still 0\n",
    "        freq[item] = np.sqrt(np.log1p(qty))\n",
    "    \n",
    "    max_freq = max(freq.values()) or 1e-6\n",
    "    freq = {item : qty/max_freq for item, qty in freq.items()} # normalization\n",
    "    # freq = {item : qty/fmax for item, qty in freq.items()} # normalization\n",
    "    # print(\"Normalized frequency: \",freq)\n",
    "    \n",
    "    ## decay factor for each item\n",
    "    K = {item : 0 for item in items['StockCode']}\n",
    "    for item in items['StockCode']:\n",
    "        K[item] = kmin + (kmax - kmin) * freq[item]\n",
    "    # print(\"Recency factor: \",K)\n",
    "    \n",
    "    purchase_date = customer_data['Last purchase date'].values[0]\n",
    "    for item, date in purchase_date.items():\n",
    "        # for each previously purchased item by customer, boost its recency bias\n",
    "        time_delta = (today - date).days\n",
    "        decay_factor = 1 + d_effect*np.exp(-K[item] * time_delta)\n",
    "        recency_bias[item] = round(recency_bias[item] * decay_factor,8)\n",
    "        \n",
    "    return recency_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c575d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "## Cell-8: Description similarity\n",
    "# vectorizer = joblib.load('../Models/vectorizer.joblib')\n",
    "\n",
    "def compute_description_similarity(current_basket, items_df=items, vectorizer=None, vectorizer_path=None):\n",
    "    \"\"\"\n",
    "    Compute description similarity (D_i) for all items w.r.t. current basket.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    descrip : pd.DataFrame\n",
    "        Must contain ['StockCode', 'Clean_Description']\n",
    "    current_basket : list of str\n",
    "        StockCodes in the current basket\n",
    "    vectorizer : sklearn.feature_extraction.text.CountVectorizer\n",
    "        Pre-trained CountVectorizer\n",
    "    vectorizer_path : str\n",
    "        Path to the pre-trained CountVectorizer joblib file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary\n",
    "        ['StockCode', 'D'] where D ‚àà [0, 1]\n",
    "    \"\"\"\n",
    "    if isinstance(current_basket, (list,set)): # only list of items\n",
    "        current_basket = set(current_basket)\n",
    "    elif hasattr(current_basket, 'StockCode') or 'StockCode' in current_basket.columns: # dataframe with other info\n",
    "        # hasattr for checking series\n",
    "        current_basket = set(current_basket['StockCode'])\n",
    "    else:\n",
    "        raise TypeError(\"current basket must be list,set or Dataframe with 'StockCode' column.\")\n",
    "    \n",
    "    descrip = items_df.copy()\n",
    "    descrip['Clean_Description'] = descrip['Description'].str.lower().str.replace('[^A-Za-z]+', ' ', regex=True).str.strip()\n",
    "    \n",
    "    if vectorizer:\n",
    "        vocab = vectorizer\n",
    "    elif vectorizer_path:\n",
    "        vocab = joblib.load(vectorizer_path)\n",
    "    else: # train vectorizer from scratch\n",
    "        # vocab = CountVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "        vocab = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "        vocab.fit(descrip['Clean_Description'])\n",
    "    \n",
    "    # tranform all items descriptions\n",
    "    item_desc_vector = vocab.transform(descrip['Clean_Description'])\n",
    "    print(vocab.get_feature_names_out())\n",
    "    # get descriptions of items in current basket\n",
    "    basket_desc = descrip.loc[descrip['StockCode'].isin(current_basket), 'Clean_Description']\n",
    "    \n",
    "    if basket_desc.empty:\n",
    "        # not items in basket or basket not given,\n",
    "        return {}\n",
    "\n",
    "    # transform descriptions from items in baskets\n",
    "    basket_vector = vocab.transform(basket_desc)\n",
    "    \n",
    "    # compute cosine similarity\n",
    "    similarity_matrix = cosine_similarity(item_desc_vector, basket_vector)\n",
    "    # print(similarity_matrix, similarity_matrix.shape)\n",
    "    # take maximum similarity to any basket items\n",
    "    \"\"\" For each item in the catalog, take the maximum similarity across all basket items.\n",
    "        Idea: ‚ÄúHow similar is this item to any item in the basket?‚Äù\n",
    "    \"\"\"\n",
    "    D = similarity_matrix.max(axis=1)\n",
    "    # D = D - D.mean()\n",
    "    # sns.displot(D)\n",
    "    \n",
    "    return dict(zip(descrip['StockCode'], D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b17449",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cell-9: Aggregative function\n",
    "Today = df['Purchase Date'].max().date() + pd.DateOffset(days=1)\n",
    "Time_period = (df['Purchase Date'].max() - df['Purchase Date'].min()).days\n",
    "\n",
    "def Recommendation(current_basket, item_data=items, customer_data=customer, association_rules=rules, \n",
    "    Coefficients=None, Id=-1, d_effect=1, current_date=Today, Time_period=Time_period, \n",
    "    budget=None, vectorizer=None, Discount=None):\n",
    "    \"\"\" Function for aggregation of all effect -> probability vector of each item.\n",
    "     Compute top-N recommended items for a given customer + basket using hybrid model.\n",
    "    \n",
    "    Args:\n",
    "        current_basket : list | set | pd.DataFrame\n",
    "            List of items in the current basket.\n",
    "        item_data: pd.DataFrame\n",
    "            Dataset containing item information.\n",
    "        customer_data: pd.DataFrame | pd.Series\n",
    "            Dataset containing customer's data\n",
    "        association_rules: pd.DataFrame\n",
    "            All association rules from Apriori algorithm\n",
    "        Coefficients: list\n",
    "            All weights associated with each factor\n",
    "        Id: int  [optional]\n",
    "            Each customer's unique identifier (in case entire data is given)\n",
    "        d_effect: float\n",
    "            strength of recency decay\n",
    "        current_date: datetime\n",
    "            date at which transactions is occuring\n",
    "        Time_period: int\n",
    "            Total time span in the dataset (for recency scaling)\n",
    "        budget: float\n",
    "            Total budget for shopping furthur\n",
    "        vectorizer: sklearn Vectorizer  [optional]\n",
    "            Text vectorizer for description similarity\n",
    "        Discount: dictionary\n",
    "            Percentage of discount for given item\n",
    "    \n",
    "    returns:\n",
    "        list of recommended items along with their data\n",
    "    \"\"\"\n",
    "    \n",
    "    if current_basket is None: # initial \n",
    "        print(\"No Item in basket.\")\n",
    "    \n",
    "    ## final items probability vector\n",
    "    all_items = item_data['StockCode'].to_list()\n",
    "    logit = {}\n",
    "    \n",
    "    # Unpack factor weights\n",
    "    if Coefficients is None:\n",
    "        Coefficients = {} # one of each feature\n",
    "    # unlock the needed coefficients\n",
    "    alpha, beta, delta, gamma, episilon = Coefficients.get('alpha',1), Coefficients.get('beta',1), Coefficients.get('delta',1), Coefficients.get('gamma',1), Coefficients.get('episilon',1)\n",
    "    eta = Coefficients.get('eta',1)\n",
    "    \n",
    "    # --- Compute each component from each function ---\n",
    "    b_i = compute_bias(item_data)\n",
    "    H_i = compute_history(customer_data, Id)\n",
    "    R_i = compute_rules(current_basket, association_rules, item_data)\n",
    "    P_i = compute_price_bias(item_data, budget)\n",
    "    \n",
    "    if Discount is None: # if discount is given\n",
    "        discount = {}\n",
    "    else:\n",
    "        discount = compute_discount_boost(Discount, item_data)\n",
    "    \n",
    "    t_i = compute_RecencyBias(item_data, customer=customer, \n",
    "        customer_id=Id, d_effect=d_effect, today=current_date, Time_period=Time_period)\n",
    "    Cosine = compute_description_similarity(current_basket, item_data, vectorizer=vectorizer)\n",
    "    \n",
    "    for i in all_items:\n",
    "        logit[i] = b_i.get(i,0) + (alpha*H_i.get(i,0) + beta*R_i.get(i,0) + delta*np.log(t_i[i]) +\n",
    "            max(gamma*P_i.get(i,0), 0) + eta*discount.get(i,0) + episilon*np.log( 1+Cosine.get(i,0) ))\n",
    "        ## clips out negetive budget.\n",
    "\n",
    "    \n",
    "    # --- Normalize logits to probabilities ---\n",
    "    # ## Using min-max scaling\n",
    "    # logits_array = np.array(list(logit.values()), dtype=float) ## strictly need to convert it to float\n",
    "    # maxx = logits_array.max()\n",
    "    # minn = logits_array.min()\n",
    "    # Probability = {k: (v - minn + 1e-6) / (maxx - minn + 1e-6) for k, v in logit.items()}\n",
    "    ## softmax normalization for probabilistic interpretation\n",
    "    Probability = Softmax_normalizer(logit) \n",
    "    \n",
    "    ## Store it in combination with item data\n",
    "    Item = item_data[['StockCode', 'Description', 'Current_Price']].copy()\n",
    "    Item['Probability'] = Item['StockCode'].map(Probability).fillna(0)\n",
    "    \n",
    "    # Sort by probability\n",
    "    Item = Item.sort_values(by='Probability', ascending=False)\n",
    "    \n",
    "    return Item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa4d25a",
   "metadata": {},
   "source": [
    "#### Part-5: Parallelize -> Scalibility\n",
    "Our model works fine currently for single basket, but we want to scale it to multiple baskets. <br>\n",
    "We need model to work in parallel for multiple baskets so that it works faster on deployment.\n",
    "\n",
    "Any real life recommendation system should be able fast, and be able to work for multiple users at the same time. <br>\n",
    "-> Hence, we now optimize all our functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d0639",
   "metadata": {},
   "source": [
    "> Create a Class is always better -> OOPS concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21c27a4",
   "metadata": {},
   "source": [
    "For compute_history_batch, for each customer we are finding effect on each item. <br>\n",
    "For our model to work, Our batch must not have baskets from same customer while training. <br>\n",
    "So, for training we plan to send in batches of baskets bought on same day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231aa54c",
   "metadata": {},
   "source": [
    "Our Final model class is working but it is way too slow. <br>\n",
    "So, we will now optimize it as much as possible for time efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35e775",
   "metadata": {},
   "source": [
    "1. The way we will find description similarity is somewhat different from above. <br>\n",
    "   Here instead of similarity b/w each item of basket with each item of catalog, we are finding similarity b/w average semantic theme of the basket with each item of catalog. This simiplifies the problem.\n",
    "2. Cosine similarity automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "import math\n",
    "\n",
    "EPS = 1e-9\n",
    "DEFAULT_HALF_LIFE_DAYS = 7\n",
    "MAGIC_KMIN_PERIOD = 30\n",
    "\n",
    "class Retail_Recommendation_optimal:\n",
    "    \"\"\"\n",
    "    Batch-capable hybrid recommender combining:\n",
    "      - item bias (popularity)\n",
    "      - per-customer purchase history affinity\n",
    "      - association-rule boosts\n",
    "      - per-basket description similarity\n",
    "      - per-basket / per-customer price-affordability (budget)\n",
    "      - recency boost per (user,item)\n",
    "      - discount boost (batch-level)\n",
    "    \n",
    "    This class computes item-level arrays once and then scores baskets in vectorized form.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, items_data, customer_data, rules, vectorizer=None, vectorizer_path=None,\n",
    "                 initial_weights=None, d_effect=1, current_date=None, Time_period=None, filter_items=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            items_df (pd.DataFrame): must contain ['StockCode','Description','Current_Price',\n",
    "                                                  'Total_quantity','Num_orders'].\n",
    "            customer_df (pd.DataFrame): rows may contain dicts in columns 'Purchase count',\n",
    "                                        'Purchase quantity', 'Last purchase date'.\n",
    "            rules_df (pd.DataFrame): cols ['antecedent','consequent','confidence','lift'].\n",
    "            vectorizer: optional pre-trained sklearn vectorizer for descriptions (TF-IDF).\n",
    "            vectorizer_path: Path to pre-trained sklearn vectorizer for descriptions (TF-IDF).\n",
    "            initial_weights: dict with keys alpha,beta,delta,gamma,epsilon,eta (defaults used if None).\n",
    "            d_effect: base recency multiplier for items (floats).\n",
    "            today: pd.Timestamp or pd.Date for \"current\" date; if None derived from items_df if possible.\n",
    "            time_period: integer days span; if None computed from items_df.\n",
    "        \"\"\"\n",
    "        # Data\n",
    "        self.item_data = items_data\n",
    "        self.customer_data = customer_data\n",
    "        self.rules = rules\n",
    "        \n",
    "        # Items initialization\n",
    "        self.all_items = np.asarray(self.item_data['StockCode'].unique())\n",
    "        self.item_to_index = {item:i for i,item in enumerate(self.all_items)}\n",
    "        self.n_items = len(self.all_items)\n",
    "        \n",
    "        self.vectorizer = vectorizer\n",
    "        self.vectorizer_path = vectorizer_path\n",
    "        \n",
    "        ## OPTIONAL: Remove rare items\n",
    "        if filter_items:\n",
    "            self._remove_rare_items()\n",
    "        \n",
    "        # Hyper-parameters & weights\n",
    "        self.d_effect = float(d_effect)\n",
    "        default_weights = {'alpha':0.5,'beta':0.4,'delta':0.1,'gamma':0.2,'epsilon':0.3,'eta':0.3}\n",
    "        self.weights = default_weights if initial_weights is None else initial_weights\n",
    "        \n",
    "        # Time params\n",
    "        self.current_date = pd.Timestamp.now().date() if current_date is None else pd.Timestamp(current_date)\n",
    "        self.Time_period = ((items_data['Last_sale'].max() - items_data['Last_sale'].min()).days \n",
    "                            if Time_period is None else Time_period)\n",
    "        \n",
    "        \n",
    "        # Caches -> Mapping for customer id\n",
    "        self.history_cache = {}\n",
    "        self.recency_cache = {}\n",
    "        \n",
    "        # # Description vectorizer\n",
    "        # if 'Clean_Description' not in self.item_data.columns:\n",
    "        #     self.item_data = self.item_data.copy()\n",
    "        #     self.item_data.loc[:,'Clean_Description'] = (self.item_data.loc[:,'Description'].str.lower().str.replace('[^A-Za-z]+',' ',regex=True).str.strip())\n",
    "        # if self.vectorizer is None:\n",
    "        #     if self.vectorizer_path:\n",
    "        #         self.vectorizer = joblib.load(self.vectorizer_path)\n",
    "        #     else:\n",
    "        #         self.vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "        #         self.vectorizer.fit(self.item_data['Clean_Description'])\n",
    "        # self.vocab = self.vectorizer.transform(self.item_data['Clean_Description'])\n",
    "        # # item-item similarity matrix -> n_item^2 memory complexity -> can remove reare items\n",
    "        # self.similarity_matrix = cosine_similarity(self.vocab, dense_output=False).astype(np.float32)\n",
    "\n",
    "        # Precompute static factors\n",
    "        self.Bias = self.compute_bias()\n",
    "        self.rules_lookup = self._build_rules_index()\n",
    "        \n",
    "        # Precompute boost for each rule\n",
    "        self._prepare_rules_boost()\n",
    "        \n",
    "\n",
    "    # ----------------- Helpers -----------------\n",
    "    @staticmethod\n",
    "    def normalize_dict(d):\n",
    "        if not d: return {}\n",
    "        max_val = max(d.values())\n",
    "        return {k:v/max_val for k,v in d.items()}\n",
    "    @staticmethod\n",
    "    def normalize_array(a):\n",
    "        amin, amax = 0, a.max()\n",
    "        return (a - amin) / (amax - amin)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_dict(x):\n",
    "        vals = np.array(list(x.values()))\n",
    "        vals = (vals - vals.min())/(vals.max()-vals.min())\n",
    "        e = np.exp(vals)\n",
    "        probs = e / e.sum()\n",
    "        return dict(zip(x.keys(), probs))\n",
    "\n",
    "        # Optional: remove rare items\n",
    "    def _remove_rare_items(self, min_orders=300, min_customers=100):\n",
    "        \"\"\"\n",
    "        Filter items to keep only those with sufficient popularity (Frequency > min_orders).\n",
    "        Also cleans customer_data dictionaries to remove rare/unused items.\n",
    "\n",
    "        Args:\n",
    "            min_orders : int\n",
    "                Minimum number of orders required for an item to be kept.\n",
    "            min_customers : int\n",
    "                Minimum number of unique customers required for an item to be kept.\n",
    "        \"\"\"\n",
    "        # remove from items_data\n",
    "        self.item_data = self.item_data[(self.item_data['Num_orders'] > min_orders) & (self.item_data['Num_customers'] > min_customers)]\n",
    "        print(\"Number of important items: \",self.item_data.shape[0])\n",
    "        \n",
    "        self.all_items = set(self.item_data['StockCode'].unique())\n",
    "        self.item_to_index = {item:i for i,item in enumerate(self.all_items)}\n",
    "        self.n_items = len(self.all_items)\n",
    "        # remove from customer data\n",
    "        for ind, cust in self.customer_data.iterrows():\n",
    "            pc = {k:v for k,v in cust['Purchase count'].items() if k in self.all_items}\n",
    "            pq = {k:v for k,v in cust['Purchase quantity'].items() if k in self.all_items}\n",
    "            self.customer_data.at[ind,'Purchase count'] = pc\n",
    "            self.customer_data.at[ind,'Purchase quantity'] = pq\n",
    "            \n",
    "        # remove from rules\n",
    "        def valid_rule(row):\n",
    "            ant = row['antecedent']\n",
    "            if not isinstance(ant, (list, tuple, set)): ant = [ant]\n",
    "            ant = set(ant)\n",
    "            \n",
    "            return ant.issubset(self.all_items)\n",
    "        self.rules = self.rules[self.rules.apply(valid_rule, axis=1)]\n",
    "\n",
    "        \n",
    "    def _build_rules_index(self):\n",
    "        lookup = defaultdict(list)\n",
    "        for _, r in self.rules.iterrows():\n",
    "            ant = tuple(sorted(r['antecedent']))\n",
    "            lookup[ant].append(r)\n",
    "        return lookup\n",
    "\n",
    "        # Precompute boost vector for each antecedents\n",
    "    def _prepare_rules_boost(self):\n",
    "        self.rules_boost = defaultdict(float)\n",
    "        for ant,row in self.rules_lookup.items(): # for each rule, compute its boost\n",
    "            boost_vector = defaultdict(float)\n",
    "            for r in row: # for each associated consequent,\n",
    "                boost_val = r['confidence'] * np.log1p(r['lift'])\n",
    "                for conseq in r['consequent']: # for each connseq\n",
    "                    boost_vector[conseq] += boost_val\n",
    "            # For each antecedent, we have boost for each consequent\n",
    "            self.rules_boost[ant] = normalize_dict(boost_vector)\n",
    "\n",
    "    # ----------------- Feature computations -----------------\n",
    "    # ----- static factors -----\n",
    "    def compute_bias(self, array=True):\n",
    "        bias = {row['StockCode']: np.log1p(row['Frequency']) for _,row in self.item_data.iterrows()}\n",
    "        if array:\n",
    "            bias = np.array([bias.get(item, 0) for item in self.all_items], dtype=np.float32)\n",
    "            return self.normalize_array(bias)\n",
    "        return self.normalize_dict(bias)\n",
    "\n",
    "    # ----- per-customer factors -----\n",
    "    def compute_history(self, cust_id):\n",
    "        if cust_id in self.history_cache:\n",
    "            return self.history_cache[cust_id]\n",
    "\n",
    "        cust_data = self.customer_data[self.customer_data['Customer ID']==cust_id]\n",
    "        if cust_data.empty:\n",
    "            self.history_cache[cust_id] = {}\n",
    "            return {}\n",
    "\n",
    "        count_dict = cust_data['Purchase count'].values[0]\n",
    "        qty_dict = cust_data['Purchase quantity'].values[0]\n",
    "        history = {item: np.log1p(qty_dict.get(item,0)/(count_dict.get(item,1))) \n",
    "                   for item in count_dict.keys()}\n",
    "        self.history_cache[cust_id] = self.normalize_dict(history)\n",
    "        return self.history_cache[cust_id]\n",
    "\n",
    "    def compute_recency(self, cust_id):\n",
    "        if cust_id in self.recency_cache:\n",
    "            return self.recency_cache[cust_id]\n",
    "\n",
    "        recency = np.zeros(self.n_items, dtype=np.float32)\n",
    "        cust_data = self.customer_data[self.customer_data['Customer ID']==cust_id]\n",
    "        if cust_data.empty:\n",
    "            self.recency_cache[cust_id] = recency\n",
    "            return recency\n",
    "\n",
    "        qty_dict = cust_data['Purchase quantity'].values[0]\n",
    "        freq = np.array([np.sqrt(np.log1p(qty_dict.get(item,0))) for item in self.all_items], dtype=np.float32)\n",
    "        freq = freq / freq.max() # normalize to fit maximum value\n",
    "        \n",
    "        kmin = np.log(2)/DEFAULT_HALF_LIFE_DAYS\n",
    "        kmax = np.log(2)/self.Time_period\n",
    "        K = kmin + (kmax-kmin)*freq\n",
    "\n",
    "        last_purchase = cust_data['Last purchase date'].values[0]\n",
    "        for i,item in enumerate(self.all_items):\n",
    "            last_date = last_purchase.get(item, None)\n",
    "            if last_date is None:\n",
    "                delta_days = self.Time_period\n",
    "            else:\n",
    "                delta_days = (self.current_date - last_purchase.get(item, None)).days # Time since last purchase -> delta=0 for fresh items\n",
    "            recency[i] = (1 + d_effect*np.exp(-K[i] * delta_days))\n",
    "        \n",
    "        self.recency_cache[cust_id] = recency\n",
    "        return recency\n",
    "\n",
    "    # ----- Discount boost -----\n",
    "    def compute_discount_boost(self, discount_dict):\n",
    "        if not discount_dict:\n",
    "            return {}\n",
    "        boost = {item: np.log1p(discount_dict[item]) for item in discount_dict}\n",
    "        return self.normalize_dict(boost)\n",
    "\n",
    "    # ----- Price boost -----\n",
    "    def compute_price_bias(self, budget):\n",
    "        if budget is None:\n",
    "            return {}\n",
    "        pb = np.array([np.log1p(budget/row['Current_Price']) for item,row in self.item_data.iterrows()], dtype=np.float32)\n",
    "        return self.normalize_array(pb)\n",
    "\n",
    "    # For each basket -> Pull out values\n",
    "    def compute_description_boost(self, baskets_df):\n",
    "        \"\"\"Compute description similarity using precomputed item-item matrix\"\"\"\n",
    "        desc_map = {}\n",
    "        for idx,row in baskets_df.iterrows(): # for each basket\n",
    "            basket_items = set(row['StockCode']) if isinstance(row['StockCode'],(list,set)) else set()\n",
    "            if not basket_items:\n",
    "                desc_map[idx] = {}\n",
    "                continue\n",
    "            \n",
    "            # Indexies of basket items\n",
    "            basket_idx  = [self.item_to_index[item] for item in basket_items if item in self.item_to_index]\n",
    "            \n",
    "            sim_vec = self.similarity_matrix[basket_idx].toarray() # similarity vector for each basket item with all other items\n",
    "            # print(sim_vec.shape)\n",
    "            avg_sim_vec = sim_vec.mean(axis=0) # average similarity for entire basket with all other items\n",
    "            # print(avg_sim_vec.shape)\n",
    "            \n",
    "            # Normalize boost\n",
    "            max_val = avg_sim_vec.max()\n",
    "            if max_val > 0:\n",
    "                desc_map[idx] = avg_sim_vec / max_val\n",
    "            else:\n",
    "                desc_map[idx] = np.zeros(self.n_items, dtype=np.float32)\n",
    "        \n",
    "        return desc_map\n",
    "    \n",
    "    # ----- Rules boost -----\n",
    "    def compute_association_boost(self, baskets):\n",
    "        rules_map = {}\n",
    "        for idx, row in baskets.iterrows():\n",
    "            current_basket = set(row['StockCode']) if isinstance(row['StockCode'], (list, set)) else set()\n",
    "            if not current_basket:\n",
    "                rules_map[idx] = {}\n",
    "                continue\n",
    "            \n",
    "            conseq_boost = {}\n",
    "            # check for each antecedent\n",
    "            for ant, boost_dict in self.rules_boost.items():\n",
    "                if set(ant).issubset(current_basket): # if ant match basket itemset\n",
    "                    for item,val in boost_dict.items():\n",
    "                        conseq_boost[item] = conseq_boost.get(item, 0) + val\n",
    "            \n",
    "            # For idx basket, we have rules boost on conseq_boost\n",
    "            rules_map[idx] = conseq_boost\n",
    "        return rules_map\n",
    "    \n",
    "            \n",
    "    # ----------------- Recommendation -----------------\n",
    "    def recommend(self, baskets, Coefficients = None, budget_dict=None, discount_dict=None, top_n=5):\n",
    "        \"\"\"\n",
    "        Aggregate all factors to compute item probabilities per basket.\n",
    "        Supports mix of dict (sparse) and array (dense) features.\n",
    "        \"\"\"\n",
    "        if Coefficients is not None:\n",
    "            self.weights = Coefficients\n",
    "        \n",
    "        results = {}\n",
    "\n",
    "        # Precompute dense features\n",
    "        # Desc_map = self.compute_description_boost(baskets) # dense arrays\n",
    "        rules_map = self.compute_association_boost(baskets) # sparse dicts\n",
    "        Discount_dict = self.compute_discount_boost(discount_dict) # sparse dict\n",
    "        price = self.compute_price_bias(budget_dict)  # sparse dict\n",
    "        \n",
    "        \n",
    "        for idx,row in tqdm.tqdm(baskets.iterrows()):\n",
    "            cid = row.get('Customer ID', -1)\n",
    "            basket_items = set(row['StockCode']) if isinstance(row['StockCode'], (list, set)) else set()\n",
    "            \n",
    "            logit = {}\n",
    "            \n",
    "            # Fetch features\n",
    "            H_i = self.compute_history(cid) # sparse dict\n",
    "            T_i = self.compute_recency(cid) # dense array\n",
    "            R_i = rules_map.get(idx, {})    # sparse dict\n",
    "            ## Discount, price will work directly\n",
    "            # Desc = Desc_map.get(idx, np.zeros(self.n_items, dtype=np.float32))\n",
    "            \n",
    "            for i, item in enumerate(self.all_items):\n",
    "                logit[i] = self.Bias[i] + self.weights['alpha']*H_i.get(item, 0.0) + self.weights['beta']*R_i.get(item, 0.0) + self.weights['eta']*Discount_dict.get(item, 0.0)\n",
    "                logit[i] += self.weights['delta']*price.get(item, 0) + self.weights['gamma']*T_i[i]\n",
    "                # logit[i] += self.weights['epsilon']*np.log1p(Desc[i])\n",
    "        \n",
    "            # Softmax normalization\n",
    "            Prob = self.softmax_dict(logit)\n",
    "            \n",
    "            # Picking out top-N items\n",
    "            all_items_list = list(self.all_items)                     # convert set ‚Üí list\n",
    "            top_idx = np.argsort(Prob)[::-1][:top_n]                  # indices of top-N\n",
    "            top_codes = [all_items_list[i] for i in top_idx]          # get item codes\n",
    "            top_probs = [Prob[i] for i in top_idx]                    # get probabilities\n",
    "\n",
    "            df_top = self.item_data[self.item_data['StockCode'].isin(top_codes)].copy()\n",
    "            df_top['Probability'] = df_top['StockCode'].map(dict(zip(top_codes, top_probs)))\n",
    "            df_top = df_top.set_index('StockCode').loc[top_codes].reset_index()\n",
    "            df_top['Rank'] = range(1, len(df_top)+1)\n",
    "            results[idx] = df_top\n",
    "\n",
    "        return results\n",
    "\n",
    "rec2 = Retail_Recommendation_optimal(items, customer, rules=rules, initial_weights=params, d_effect=1,\n",
    "    current_date=Today, Time_period=100, vectorizer=vectorizer, filter_items=True)\n",
    "basket1 = baskets.iloc[[2]]\n",
    "# rec2.recommend(baskets, budget_dict=None, discount_dict=None, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3d84a",
   "metadata": {},
   "source": [
    "Even with the Best Structure from python, Our model is way too slow. <br>\n",
    "So, for optimization -> \n",
    "1. We will stop using python loops. Only work in C loops indirectly\n",
    "2. Using python dictionarys is way too slow. We will use sparse arrays instead.\n",
    "3. Only numpy vectorised array so that computations become extremely fast.\n",
    "4. Using sorting on dictionary in recommend functionis O[n logn]. So, instead we use argpartition + argsort -> O[n], fully in C.\n",
    "5. Using pandas DataFrame is way too inefficient for middle steps. Completely replace them with arrays.\n",
    "6. Similar description is a relevant factors, but similarity score is a O[n_item^2] operation. -> Too complex. <br>\n",
    "   It would be better to just remove it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e1b0d",
   "metadata": {},
   "source": [
    "Now, we will Code the most optimal recommendation class that i could make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "EPS = 1e-9\n",
    "DEFAULT_HALF_LIFE_DAYS = 7\n",
    "\n",
    "class Retail_Recommendation_Fast:\n",
    "    \"\"\"\n",
    "    Extremely optimized version (description boost removed).\n",
    "    Uses full NumPy vectorization, cached arrays, and minimal loops.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, items_data, customer_data, rules, vectorizer=None, vectorizer_path=None,\n",
    "                 initial_weights=None, d_effect=1, current_date=None, Time_period=None, \n",
    "                 filter_items=False, include_description=False):\n",
    "\n",
    "        self.item_data = items_data.copy()\n",
    "        self.customer_data = customer_data\n",
    "        self.rules = rules\n",
    "\n",
    "        # Items setup\n",
    "        if filter_items:\n",
    "            self._remove_rare_items()\n",
    "        self.all_items = np.array(self.item_data['StockCode'].unique())\n",
    "        self.item_to_index = {item: i for i, item in enumerate(self.all_items)}\n",
    "        self.n_items = len(self.all_items)\n",
    "\n",
    "        # Time and weights\n",
    "        self.d_effect = float(d_effect)\n",
    "        self.weights = (\n",
    "            {'alpha': 0.5, 'beta': 0.4, 'delta': 0.1,\n",
    "             'gamma': 0.2, 'eta': 0.3}\n",
    "            if initial_weights is None else initial_weights\n",
    "        )\n",
    "        self.current_date = (\n",
    "            pd.Timestamp.now().date()\n",
    "            if current_date is None else pd.Timestamp(current_date)\n",
    "        )\n",
    "        self.Time_period = (\n",
    "            pd.Timedelta(days=100) # (items_data['Last_sale'].max() - items_data['Last_sale'].min()).days\n",
    "            if Time_period is None else Time_period\n",
    "        )\n",
    "\n",
    "        # Caches\n",
    "        self.history_cache = {}\n",
    "        self.recency_cache = {}\n",
    "\n",
    "        # Precompute static arrays\n",
    "        self.Bias = self._compute_bias()\n",
    "        self.rules_lookup = self._build_rules_index()\n",
    "        self.rules_boost = self._prepare_rules_boost()\n",
    "        \n",
    "        self.include_description = include_description\n",
    "        \n",
    "        if self.include_description:\n",
    "                # ----------------- Description similarity -----------------\n",
    "            if 'Clean_Description' not in self.item_data.columns:\n",
    "                self.item_data = self.item_data.copy()\n",
    "                self.item_data['Clean_Description'] = (self.item_data['Description'].str.lower().str.replace('[^A-Za-z]+',' ', regex=True).str.strip())\n",
    "            if vectorizer is None:\n",
    "                if vectorizer_path:\n",
    "                    import joblib\n",
    "                    vectorizer = joblib.load(vectorizer_path)\n",
    "                else:\n",
    "                    vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "                    vectorizer.fit(self.item_data['Clean_Description'])\n",
    "            self.vectorizer = vectorizer\n",
    "\n",
    "            self.vocab = self.vectorizer.transform(self.item_data['Clean_Description'])\n",
    "            self.similarity_matrix = cosine_similarity(self.vocab, dense_output=False).astype(np.float32)\n",
    "        \n",
    "        \n",
    "\n",
    "    # ------------------ Helper methods ------------------\n",
    "    def _remove_rare_items(self, min_orders=300, min_customers=100):\n",
    "        \"\"\"\n",
    "        Filter items to keep only those with sufficient popularity (Frequency > min_orders).\n",
    "        Also cleans customer_data dictionaries to remove rare/unused items.\n",
    "\n",
    "        Args:\n",
    "            min_orders : int\n",
    "                Minimum number of orders required for an item to be kept.\n",
    "            min_customers : int\n",
    "                Minimum number of unique customers required for an item to be kept.\n",
    "        \"\"\"\n",
    "        df = self.item_data\n",
    "        df = df[(df['Num_orders'] > min_orders) & (df['Num_customers'] > min_customers)]\n",
    "        self.item_data = df.reset_index(drop=True)\n",
    "        print(f\"Number of important items: {self.item_data.shape[0]}\")\n",
    "        \n",
    "\n",
    "    def _build_rules_index(self):\n",
    "        lookup = defaultdict(list)\n",
    "        for _, r in self.rules.iterrows():\n",
    "            ant = tuple(sorted(r['antecedent']))\n",
    "            lookup[ant].append(r)\n",
    "        return lookup\n",
    "\n",
    "    def _prepare_rules_boost(self):\n",
    "        rb = {}\n",
    "        for ant, rows in self.rules_lookup.items():\n",
    "            boost_vector = defaultdict(float)\n",
    "            for r in rows:\n",
    "                boost_val = r['confidence'] * math.log1p(r['lift'])\n",
    "                for conseq in r['consequent']:\n",
    "                    boost_vector[conseq] += boost_val\n",
    "            # Normalize\n",
    "            max_val = max(boost_vector.values(), default=1)\n",
    "            rb[ant] = {k: v / max_val for k, v in boost_vector.items()}\n",
    "        return rb\n",
    "\n",
    "    def _compute_bias(self):\n",
    "        freq = self.item_data['Frequency'].values\n",
    "        bias = np.log1p(freq).astype(np.float32)\n",
    "        return bias / bias.max()\n",
    "\n",
    "    # ------------------ Feature computations ------------------\n",
    "    def compute_history(self, cust_id):\n",
    "        if cust_id in self.history_cache:\n",
    "            return self.history_cache[cust_id]\n",
    "\n",
    "        cust_data = self.customer_data[self.customer_data['Customer ID'] == cust_id]\n",
    "        hist = np.zeros(self.n_items, dtype=np.float32)\n",
    "        if cust_data.empty:\n",
    "            self.history_cache[cust_id] = hist\n",
    "            return hist\n",
    "\n",
    "        count = cust_data.iloc[0]['Purchase count']\n",
    "        qty = cust_data.iloc[0]['Purchase quantity']\n",
    "\n",
    "        for item, q in qty.items():\n",
    "            idx = self.item_to_index.get(item)\n",
    "            if idx is not None:\n",
    "                c = count.get(item, 1)\n",
    "                hist[idx] = math.log1p(q / c)\n",
    "\n",
    "        if hist.max() > 0:\n",
    "            hist /= hist.max()\n",
    "        self.history_cache[cust_id] = hist\n",
    "        return hist\n",
    "\n",
    "    def compute_recency(self, cust_id):\n",
    "        if cust_id in self.recency_cache:\n",
    "            return self.recency_cache[cust_id]\n",
    "\n",
    "        cust_data = self.customer_data[self.customer_data['Customer ID'] == cust_id]\n",
    "        rec = np.ones(self.n_items, dtype=np.float32)\n",
    "        if cust_data.empty:\n",
    "            self.recency_cache[cust_id] = rec\n",
    "            return rec\n",
    "\n",
    "        qty_dict = cust_data.iloc[0]['Purchase quantity']\n",
    "        freq = np.array([math.sqrt(math.log1p(qty_dict.get(it, 0))) for it in self.all_items], dtype=np.float32)\n",
    "        freq /= (freq.max() + EPS)\n",
    "\n",
    "        kmin = np.log(2) / DEFAULT_HALF_LIFE_DAYS\n",
    "        kmax = np.log(2) / self.Time_period\n",
    "        K = kmin + (kmax - kmin) * freq\n",
    "\n",
    "        last_purchase = cust_data.iloc[0]['Last purchase date']\n",
    "        deltas = np.array(\n",
    "            [(self.current_date - last_purchase.get(it, self.current_date)).days for it in self.all_items],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        rec = 1 + self.d_effect * np.exp(-K * deltas)\n",
    "        self.recency_cache[cust_id] = rec\n",
    "        return rec\n",
    "\n",
    "    def compute_rules_array(self, basket):\n",
    "        rules_arr = np.zeros(self.n_items, dtype=np.float32)\n",
    "        current = set(basket)\n",
    "        for ant, boost_dict in self.rules_boost.items():\n",
    "            if set(ant).issubset(current):\n",
    "                for item, val in boost_dict.items():\n",
    "                    idx = self.item_to_index.get(item)\n",
    "                    if idx is not None:\n",
    "                        rules_arr[idx] += val\n",
    "        if rules_arr.max() > 0:\n",
    "            rules_arr /= rules_arr.max()\n",
    "        return rules_arr\n",
    "\n",
    "    def compute_discount_array(self, discount_dict):\n",
    "        arr = np.zeros(self.n_items, dtype=np.float32)\n",
    "        if discount_dict:\n",
    "            for item, val in discount_dict.items():\n",
    "                idx = self.item_to_index.get(item)\n",
    "                if idx is not None:\n",
    "                    arr[idx] = math.log1p(val)\n",
    "        if arr.max() > 0:\n",
    "            arr /= arr.max()\n",
    "        return arr\n",
    "\n",
    "    def compute_price_array(self, budget):\n",
    "        arr = np.zeros(self.n_items, dtype=np.float32)\n",
    "        if budget is None:\n",
    "            return arr\n",
    "        prices = self.item_data['Current Price'].values\n",
    "        arr = np.log1p(budget / (prices + EPS)).astype(np.float32)\n",
    "        return arr / (arr.max() + EPS)\n",
    "    \n",
    "    def compute_description_boost(self, baskets_df):\n",
    "        desc_map = {}\n",
    "        for idx, row in baskets_df.iterrows():\n",
    "            basket_items = set(row['StockCode']) if isinstance(row['StockCode'], (list, set)) else set()\n",
    "            if not basket_items:\n",
    "                desc_map[idx] = np.zeros(self.n_items, dtype=np.float32)\n",
    "                continue\n",
    "            basket_idx = [self.item_to_index[item] for item in basket_items if item in self.item_to_index]\n",
    "            sim_vec = self.similarity_matrix[basket_idx].toarray()  # shape: len(basket) x n_items\n",
    "            avg_vec = sim_vec.mean(axis=0)\n",
    "            max_val = avg_vec.max()\n",
    "            desc_map[idx] = avg_vec / max_val if max_val>0 else np.zeros(self.n_items, dtype=np.float32)\n",
    "        return desc_map\n",
    "\n",
    "    # ------------------ Recommendation ------------------\n",
    "    def recommend(self, baskets, budget_dict=None, discount_dict=None, top_n=5):\n",
    "        \" If top_n == -1, then return all items \"\n",
    "        results = {}\n",
    "        discount_arr = self.compute_discount_array(discount_dict)\n",
    "        price_arr = self.compute_price_array(budget_dict)\n",
    "\n",
    "\n",
    "        if self.include_description:\n",
    "            Desc_map = self.compute_description_boost(baskets)\n",
    "\n",
    "        for idx, row in tqdm.tqdm(baskets.iterrows(), total=len(baskets)):\n",
    "            cid = row.get('Customer ID', -1)\n",
    "            basket_items = row['StockCode'] if isinstance(row['StockCode'], (list, set)) else []\n",
    "\n",
    "            # Precompute all dense feature arrays\n",
    "            H = self.compute_history(cid)\n",
    "            T = self.compute_recency(cid)\n",
    "            R = self.compute_rules_array(basket_items)\n",
    "            \n",
    "            if self.include_description:\n",
    "                Desc = Desc_map.get(idx, np.zeros(self.n_items, dtype=np.float32))\n",
    "\n",
    "            # Vectorized scoring\n",
    "            logit = (\n",
    "                self.Bias\n",
    "                + self.weights['alpha'] * H\n",
    "                + self.weights['beta'] * R\n",
    "                + self.weights['eta'] * discount_arr\n",
    "                + self.weights['delta'] * price_arr\n",
    "                + self.weights['gamma'] * T\n",
    "            )\n",
    "\n",
    "            if self.include_description:\n",
    "                logit += self.weights['epsilon'] * Desc\n",
    "\n",
    "            # Softmax probability\n",
    "            exps = np.exp(logit - logit.max())\n",
    "            prob = exps / (exps.sum() + EPS)\n",
    "\n",
    "            # Top-N items\n",
    "            if top_n == -1:\n",
    "                top_idx = np.argpartition(prob)\n",
    "                \n",
    "            top_idx = np.argpartition(prob, -top_n)[-top_n:]\n",
    "            top_idx = top_idx[np.argsort(prob[top_idx])[::-1]]\n",
    "            top_codes = self.all_items[top_idx]\n",
    "            top_probs = prob[top_idx]\n",
    "\n",
    "            df_top = (\n",
    "                self.item_data[self.item_data['StockCode'].isin(top_codes)]\n",
    "                .copy()\n",
    "                .set_index('StockCode')\n",
    "                .loc[top_codes]\n",
    "                .reset_index()\n",
    "            )\n",
    "            df_top['Probability'] = top_probs\n",
    "            df_top['Rank'] = np.arange(1, len(df_top) + 1)\n",
    "            results[idx] = df_top\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd081051",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_fast = Retail_Recommendation_Fast(\n",
    "    items_data=items,\n",
    "    customer_data=customer,\n",
    "    rules=rules,\n",
    "    initial_weights=params,\n",
    "    d_effect=1,\n",
    "    current_date=Today,\n",
    "    Time_period=100,\n",
    "    filter_items=True,\n",
    "    include_description=False\n",
    ")\n",
    "basket1 = baskets.iloc[[2]]\n",
    "\n",
    "start = time.time()\n",
    "result = rec_fast.recommend(basket1, budget_dict=None, discount_dict=None, top_n=5)\n",
    "print(\"Without description: \",time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_fast = Retail_Recommendation_Fast(\n",
    "    items_data=items,\n",
    "    customer_data=customer,\n",
    "    rules=rules,\n",
    "    initial_weights=params,\n",
    "    d_effect=1,\n",
    "    current_date=Today,\n",
    "    Time_period=100,\n",
    "    filter_items=False,\n",
    "    include_description=True\n",
    ")\n",
    "basket1 = baskets.iloc[[2]]\n",
    "\n",
    "start2 = time.time()\n",
    "result = rec_fast.recommend(basket1, budget_dict=None, discount_dict=None, top_n=5)\n",
    "print(\"With description: \",time.time() - start2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ddf5c7",
   "metadata": {},
   "source": [
    "### Saving model:\n",
    "For saving this model, we will make a `Model.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a5d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
